{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3cb6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Process_genome_name(genome_location):\n",
    "    genome  = SeqIO.parse(genome_location, 'fasta')\n",
    "    output = ''\n",
    "    for record in genome:\n",
    "        output+=f\">{record.id.split('.')[0]}\\n{record.seq}\"\n",
    "    \n",
    "    with open(genome_location, 'w') as out_file:\n",
    "        out_file.write(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4348c46b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n",
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:20:28.969425\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [reverent_hoover] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[ba/296f21] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[ba/296f21] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 85 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [distracted_miescher] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[19/e96884] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[19/e96884] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[19/e96884] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 10 chain IDs\n",
      "Wrote output to 10 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 10 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 10 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [friendly_sanger] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[27/99bec9] process > execute_jobs (1) [  0%] 0 of 10\n",
      "\n",
      "executor >  local (2)\n",
      "[27/99bec9] process > execute_jobs (1) [ 10%] 1 of 10\n",
      "\n",
      "executor >  local (3)\n",
      "[46/9dbf84] process > execute_jobs (2) [ 20%] 2 of 10\n",
      "\n",
      "executor >  local (4)\n",
      "[60/882db7] process > execute_jobs (3) [ 30%] 3 of 10\n",
      "\n",
      "executor >  local (5)\n",
      "[64/bc833c] process > execute_jobs (6) [ 40%] 4 of 10\n",
      "\n",
      "executor >  local (5)\n",
      "[64/bc833c] process > execute_jobs (6) [ 40%] 4 of 10\n",
      "\n",
      "executor >  local (6)\n",
      "[be/2bff22] process > execute_jobs (5) [ 50%] 5 of 10\n",
      "\n",
      "executor >  local (7)\n",
      "[1b/e51d6e] process > execute_jobs (7) [ 60%] 6 of 10\n",
      "\n",
      "executor >  local (7)\n",
      "[1b/e51d6e] process > execute_jobs (7) [ 60%] 6 of 10\n",
      "\n",
      "executor >  local (8)\n",
      "[4c/cc91c0] process > execute_jobs (8) [ 70%] 7 of 10\n",
      "\n",
      "executor >  local (8)\n",
      "[4c/cc91c0] process > execute_jobs (8) [ 70%] 7 of 10\n",
      "\n",
      "executor >  local (9)\n",
      "[0a/7d9f2c] process > execute_jobs (9) [ 80%] 8 of 10\n",
      "\n",
      "executor >  local (10)\n",
      "[dd/58651b] process > execute_jobs (10) [ 90%] 9 of 10\n",
      "\n",
      "executor >  local (10)\n",
      "[dd/58651b] process > execute_jobs (10) [ 90%] 9 of 10\n",
      "\n",
      "executor >  local (10)\n",
      "[dd/58651b] process > execute_jobs (10) [100%] 10 of 10 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:29.585949\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_21.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.087640\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712568983\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712568983\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:28.253387\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 1\n",
      "* paralogs: 0\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 1 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 1 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 1 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_21.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_21.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: ORTH\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 0 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.191211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: FR989951\t100022\t135723\tPeriod.1\t1000\t-\t100022\t135723\t0,0,0\t22\t80,143,101,170,140,241,74,95,192,113,133,152,113,108,228,237,84,131,151,74,111,21,\t0,1378,1615,3573,5782,9645,10495,11422,14404,15423,17443,18064,19145,19367,19967,20695,23543,24350,26923,29374,30221,35680,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 5 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:23:01.138416\n",
      "gene_loss_summary: 0 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as L: % intact 0.16857899382171226 < 0.2\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class L assigned color in the bed file: 255,50,50\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as L :: child projections classes: ['L']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.092887\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 0 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'PI', 'I', 'UL'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:02:00.622084\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n",
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:23:04.385410\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [agitated_babbage] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[92/42dbe5] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[92/42dbe5] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[92/42dbe5] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 109 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [thirsty_picasso] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[1c/7a6d71] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[1c/7a6d71] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 7 chain IDs\n",
      "Wrote output to 7 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 7 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 7 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [big_brown] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[84/af4565] process > execute_jobs (4) [  0%] 0 of 7\n",
      "\n",
      "executor >  local (2)\n",
      "[84/af4565] process > execute_jobs (4) [ 14%] 1 of 7\n",
      "\n",
      "executor >  local (2)\n",
      "[e1/e3bc66] process > execute_jobs (3) [ 28%] 2 of 7\n",
      "\n",
      "executor >  local (3)\n",
      "[ca/0ca802] process > execute_jobs (2) [ 28%] 2 of 7\n",
      "\n",
      "executor >  local (4)\n",
      "[5a/1fe828] process > execute_jobs (1) [ 42%] 3 of 7\n",
      "\n",
      "executor >  local (5)\n",
      "[91/e9e65e] process > execute_jobs (6) [ 57%] 4 of 7\n",
      "\n",
      "executor >  local (6)\n",
      "[03/fbabfd] process > execute_jobs (5) [ 71%] 5 of 7\n",
      "\n",
      "executor >  local (7)\n",
      "[bd/7d59b6] process > execute_jobs (7) [ 85%] 6 of 7\n",
      "\n",
      "executor >  local (7)\n",
      "[bd/7d59b6] process > execute_jobs (7) [100%] 7 of 7 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:33.443850\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 3 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_23.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.219825\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712569150\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712569150\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:37.047107\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 1\n",
      "* paralogs: 0\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 1 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 1 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 2 chains\n",
      "stitch fragments: processing 1 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_23.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_23.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: ORTH\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 0 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.155012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: OU538732\t100049\t118881\tPeriod.1\t1000\t-\t100049\t118881\t0,0,0\t21\t92,134,101,56,155,196,83,162,140,118,149,113,87,238,217,92,134,64,166,86,145,\t0,358,1920,2831,3558,4750,5823,7377,7791,8905,9557,10655,11110,11678,12417,13383,13936,15480,17374,18420,18687,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 6 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:25:49.998187\n",
      "gene_loss_summary: 0 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as L: % intact 0.13945278022947927 < 0.2\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class L assigned color in the bed file: 255,50,50\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as L :: child projections classes: ['L']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.065574\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 0 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'UL', 'I', 'PI'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:02:09.285945\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:25:56.633660\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [exotic_williams] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[80/b1de15] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[80/b1de15] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 103 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [kickass_angela] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[84/6ac5b5] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[84/6ac5b5] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[84/6ac5b5] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 5 chain IDs\n",
      "Wrote output to 5 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 5 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 5 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [shrivelled_meninsky] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 5\n",
      "\n",
      "executor >  local (1)\n",
      "[62/87b3c6] process > execute_jobs (2) [  0%] 0 of 5\n",
      "\n",
      "executor >  local (2)\n",
      "[62/87b3c6] process > execute_jobs (2) [ 20%] 1 of 5\n",
      "\n",
      "executor >  local (2)\n",
      "[94/d3c3ee] process > execute_jobs (1) [ 40%] 2 of 5\n",
      "\n",
      "executor >  local (3)\n",
      "[56/014fe1] process > execute_jobs (3) [ 40%] 2 of 5\n",
      "\n",
      "executor >  local (4)\n",
      "[a1/715a38] process > execute_jobs (4) [ 60%] 3 of 5\n",
      "\n",
      "executor >  local (4)\n",
      "[a1/715a38] process > execute_jobs (4) [ 80%] 4 of 5\n",
      "\n",
      "executor >  local (5)\n",
      "[89/375953] process > execute_jobs (5) [ 80%] 4 of 5\n",
      "\n",
      "executor >  local (5)\n",
      "[89/375953] process > execute_jobs (5) [100%] 5 of 5 ✔\n",
      "\n",
      "executor >  local (5)\n",
      "[89/375953] process > execute_jobs (5) [100%] 5 of 5 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:29.514054\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 3 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_26.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.087450\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712569305\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712569305\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:23.688885\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 1\n",
      "* paralogs: 0\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 1 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 1 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 2 chains\n",
      "stitch fragments: processing 1 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_26.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_26.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: ORTH\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 0 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.148079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: OX637275\t100001\t115551\tPeriod.1\t1000\t-\t100001\t115551\t0,0,0\t22\t107,143,101,209,134,242,80,183,146,102,91,149,119,99,228,216,45,137,44,154,83,81,\t0,899,1439,2269,3003,4116,4867,5957,7634,8395,8722,9403,10031,10323,10658,11447,12079,12687,12925,14174,14438,15469,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 5 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:28:23.848687\n",
      "gene_loss_summary: 0 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as L: % intact 0.10070671378091872 < 0.2\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class L assigned color in the bed file: 255,50,50\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as L :: child projections classes: ['L']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.061701\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 0 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'I', 'PI', 'UL'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:55.821154\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:28:26.661552\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [disturbed_neumann] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[f6/e2e19a] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[f6/e2e19a] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n",
      "executor >  local (1)\n",
      "[f6/e2e19a] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 34 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [nostalgic_hopper] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[cc/6dab99] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[cc/6dab99] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 27 chain IDs\n",
      "Wrote output to 27 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 27 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 27 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [maniac_fermi] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[07/4ab0a0] process > execute_jobs (1) [  0%] 0 of 21\n",
      "\n",
      "executor >  local (1)\n",
      "[07/4ab0a0] process > execute_jobs (1) [  0%] 0 of 27\n",
      "\n",
      "executor >  local (2)\n",
      "[07/4ab0a0] process > execute_jobs (1) [  3%] 1 of 27\n",
      "\n",
      "executor >  local (3)\n",
      "[4c/71df9f] process > execute_jobs (4) [  7%] 2 of 27\n",
      "\n",
      "executor >  local (4)\n",
      "[a9/fd29b0] process > execute_jobs (3) [ 11%] 3 of 27\n",
      "\n",
      "executor >  local (4)\n",
      "[a9/fd29b0] process > execute_jobs (3) [ 11%] 3 of 27\n",
      "\n",
      "executor >  local (5)\n",
      "[77/a61dd2] process > execute_jobs (6) [ 14%] 4 of 27\n",
      "\n",
      "executor >  local (5)\n",
      "[77/a61dd2] process > execute_jobs (6) [ 18%] 5 of 27\n",
      "\n",
      "executor >  local (6)\n",
      "[b3/0e72fc] process > execute_jobs (7) [ 18%] 5 of 27\n",
      "\n",
      "executor >  local (7)\n",
      "[b6/850de7] process > execute_jobs (5) [ 22%] 6 of 27\n",
      "\n",
      "executor >  local (8)\n",
      "[34/5fa381] process > execute_jobs (8) [ 25%] 7 of 27\n",
      "\n",
      "executor >  local (9)\n",
      "[d7/fad91b] process > execute_jobs (9) [ 29%] 8 of 27\n",
      "\n",
      "executor >  local (10)\n",
      "[be/99cdb8] process > execute_jobs (10) [ 33%] 9 of 27\n",
      "\n",
      "executor >  local (11)\n",
      "[37/c80754] process > execute_jobs (12) [ 37%] 10 of 27\n",
      "\n",
      "executor >  local (12)\n",
      "[91/c5cd67] process > execute_jobs (11) [ 40%] 11 of 27\n",
      "\n",
      "executor >  local (13)\n",
      "[75/73a86e] process > execute_jobs (13) [ 44%] 12 of 27\n",
      "\n",
      "executor >  local (14)\n",
      "[85/227754] process > execute_jobs (14) [ 48%] 13 of 27\n",
      "\n",
      "executor >  local (15)\n",
      "[f1/5dc47b] process > execute_jobs (15) [ 51%] 14 of 27\n",
      "\n",
      "executor >  local (16)\n",
      "[30/ac5ca0] process > execute_jobs (16) [ 55%] 15 of 27\n",
      "\n",
      "executor >  local (16)\n",
      "[30/ac5ca0] process > execute_jobs (16) [ 55%] 15 of 27\n",
      "\n",
      "executor >  local (17)\n",
      "[7a/e928a1] process > execute_jobs (17) [ 59%] 16 of 27\n",
      "\n",
      "executor >  local (18)\n",
      "[8c/b9be5f] process > execute_jobs (18) [ 62%] 17 of 27\n",
      "\n",
      "executor >  local (18)\n",
      "[8c/b9be5f] process > execute_jobs (18) [ 62%] 17 of 27\n",
      "\n",
      "executor >  local (19)\n",
      "[a2/8601be] process > execute_jobs (19) [ 66%] 18 of 27\n",
      "\n",
      "executor >  local (20)\n",
      "[12/42cd71] process > execute_jobs (21) [ 70%] 19 of 27\n",
      "\n",
      "executor >  local (21)\n",
      "[7f/823184] process > execute_jobs (22) [ 74%] 20 of 27\n",
      "\n",
      "executor >  local (22)\n",
      "[41/063bba] process > execute_jobs (23) [ 77%] 21 of 27\n",
      "\n",
      "executor >  local (23)\n",
      "[33/237e08] process > execute_jobs (20) [ 81%] 22 of 27\n",
      "\n",
      "executor >  local (24)\n",
      "[1e/bb1484] process > execute_jobs (25) [ 85%] 23 of 27\n",
      "\n",
      "executor >  local (25)\n",
      "[6a/cd2d99] process > execute_jobs (24) [ 88%] 24 of 27\n",
      "\n",
      "executor >  local (26)\n",
      "[d4/a7cd16] process > execute_jobs (27) [ 92%] 25 of 27\n",
      "\n",
      "executor >  local (27)\n",
      "[0e/44a36e] process > execute_jobs (26) [ 96%] 26 of 27\n",
      "\n",
      "executor >  local (27)\n",
      "[0e/44a36e] process > execute_jobs (26) [100%] 27 of 27 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:35.281064\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 12 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_29.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.194498\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712569466\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712569466\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:29.085587\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 1\n",
      "* paralogs: 0\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 1 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 1 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 11 chains\n",
      "stitch fragments: processing 1 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_29.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_29.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* selected chain class to annotate transcript Period: ORTH\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 8.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 0 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.320143\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: NC_059680\t98094\t127512\tPeriod.1\t1000\t+\t98094\t127512\t0,0,0\t27\t51,117,83,163,97,47,131,75,221,228,111,143,149,136,116,140,177,147,97,107,241,155,152,143,101,167,98,\t0,1880,2529,4364,6290,6822,7074,8929,9411,9865,10548,11320,14463,15288,16013,16678,17405,18906,19365,19573,19777,22990,23663,24567,24947,25572,29320,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 0 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:31:05.279614\n",
      "gene_loss_summary: 0 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as L: % intact 0.11257695690413369 < 0.2\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class L assigned color in the bed file: 255,50,50\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as L :: child projections classes: ['L']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.066533\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 0 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'I', 'UL', 'PI'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOGA pipeline is done in 0:02:01.361687\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n",
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:31:08.238730\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [condescending_booth] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[ba/e5457e] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[ba/e5457e] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 108 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [adoring_faraday] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[50/3007c6] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[50/3007c6] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 19 chain IDs\n",
      "Wrote output to 19 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 19 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 19 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [happy_faggin] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 4\n",
      "\n",
      "executor >  local (1)\n",
      "[ef/ac77de] process > execute_jobs (3) [  0%] 0 of 19\n",
      "\n",
      "executor >  local (1)\n",
      "[ef/ac77de] process > execute_jobs (3) [  0%] 0 of 19\n",
      "\n",
      "executor >  local (2)\n",
      "[ef/ac77de] process > execute_jobs (3) [  5%] 1 of 19\n",
      "\n",
      "executor >  local (2)\n",
      "[ef/ac77de] process > execute_jobs (3) [  5%] 1 of 19\n",
      "\n",
      "executor >  local (2)\n",
      "[ff/54c61a] process > execute_jobs (2) [ 10%] 2 of 19\n",
      "\n",
      "executor >  local (3)\n",
      "[fe/c6d8cc] process > execute_jobs (1) [ 10%] 2 of 19\n",
      "\n",
      "executor >  local (4)\n",
      "[a1/e7008f] process > execute_jobs (4) [ 15%] 3 of 19\n",
      "\n",
      "executor >  local (4)\n",
      "[a1/e7008f] process > execute_jobs (4) [ 21%] 4 of 19\n",
      "\n",
      "executor >  local (5)\n",
      "[89/f066df] process > execute_jobs (6) [ 21%] 4 of 19\n",
      "\n",
      "executor >  local (6)\n",
      "[f0/7c55ac] process > execute_jobs (7) [ 26%] 5 of 19\n",
      "\n",
      "executor >  local (7)\n",
      "[4b/39ce39] process > execute_jobs (5) [ 31%] 6 of 19\n",
      "\n",
      "executor >  local (7)\n",
      "[4b/39ce39] process > execute_jobs (5) [ 31%] 6 of 19\n",
      "\n",
      "executor >  local (8)\n",
      "[a7/308af2] process > execute_jobs (8) [ 36%] 7 of 19\n",
      "\n",
      "executor >  local (8)\n",
      "[a7/308af2] process > execute_jobs (8) [ 36%] 7 of 19\n",
      "\n",
      "executor >  local (9)\n",
      "[70/811819] process > execute_jobs (9) [ 42%] 8 of 19\n",
      "\n",
      "executor >  local (9)\n",
      "[70/811819] process > execute_jobs (9) [ 42%] 8 of 19\n",
      "\n",
      "executor >  local (10)\n",
      "[fc/2e37a9] process > execute_jobs (10) [ 47%] 9 of 19\n",
      "\n",
      "executor >  local (10)\n",
      "[fc/2e37a9] process > execute_jobs (10) [ 47%] 9 of 19\n",
      "\n",
      "executor >  local (11)\n",
      "[aa/776994] process > execute_jobs (12) [ 52%] 10 of 19\n",
      "\n",
      "executor >  local (11)\n",
      "[aa/776994] process > execute_jobs (12) [ 52%] 10 of 19\n",
      "\n",
      "executor >  local (12)\n",
      "[50/e0436f] process > execute_jobs (11) [ 57%] 11 of 19\n",
      "\n",
      "executor >  local (12)\n",
      "[50/e0436f] process > execute_jobs (11) [ 57%] 11 of 19\n",
      "\n",
      "executor >  local (13)\n",
      "[08/f02950] process > execute_jobs (13) [ 63%] 12 of 19\n",
      "\n",
      "executor >  local (13)\n",
      "[08/f02950] process > execute_jobs (13) [ 63%] 12 of 19\n",
      "\n",
      "executor >  local (14)\n",
      "[64/ad03e3] process > execute_jobs (14) [ 68%] 13 of 19\n",
      "\n",
      "executor >  local (15)\n",
      "[b1/4fd356] process > execute_jobs (15) [ 73%] 14 of 19\n",
      "\n",
      "executor >  local (16)\n",
      "[a7/4ce35e] process > execute_jobs (16) [ 78%] 15 of 19\n",
      "\n",
      "executor >  local (17)\n",
      "[17/86b4eb] process > execute_jobs (17) [ 84%] 16 of 19\n",
      "\n",
      "executor >  local (18)\n",
      "[66/0236e5] process > execute_jobs (19) [ 89%] 17 of 19\n",
      "\n",
      "executor >  local (18)\n",
      "[66/0236e5] process > execute_jobs (19) [ 94%] 18 of 19\n",
      "\n",
      "executor >  local (19)\n",
      "[d9/5d2f98] process > execute_jobs (18) [ 94%] 18 of 19\n",
      "\n",
      "executor >  local (19)\n",
      "[d9/5d2f98] process > execute_jobs (18) [100%] 19 of 19 ✔\n",
      "\n",
      "executor >  local (19)\n",
      "[d9/5d2f98] process > execute_jobs (18) [100%] 19 of 19 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:33.816306\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 5 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_31.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 2 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 2 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 2 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.100180\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712569632\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712569632\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 2 result files to combine\n",
      "merge_chains_output: got 2 keys in chain_genes_data\n",
      "merge_chains_output: got 2 keys in chain_raw_data\n",
      "merge_chains_output: There were 2 transcript lines and 2 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:34.663770\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 2\n",
      "classify_chains: total number of transcripts: 2\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 1\n",
      "* paralogs: 0\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 1 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 1 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 4 chains\n",
      "stitch fragments: processing 1 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_31.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_31.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: ORTH\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 0 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.147494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: CM054800\t97134\t125598\tPeriod.1\t1000\t+\t97134\t125598\t0,0,0\t27\t21,111,83,163,103,47,131,75,215,247,105,113,150,148,113,140,177,135,97,122,298,155,165,98,104,155,71,\t0,2840,3967,8922,10665,11001,11335,13798,14259,14700,15465,15823,18506,19008,19206,19767,20271,20855,21314,21530,21767,24641,25289,26078,26727,27319,28393,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 0 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:33:50.630057\n",
      "gene_loss_summary: 0 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as L: % intact 0.14096916299559473 < 0.2\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class L assigned color in the bed file: 255,50,50\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as L :: child projections classes: ['L']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.052906\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 0 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'I', 'UL', 'PI'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:02:06.740924\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:33:53.353296\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [berserk_bardeen] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[9a/e2401f] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[9a/e2401f] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[9a/e2401f] process > execute_jobs (1) [  0%] 0 of 1 ✔\n",
      "\n",
      "executor >  local (1)\n",
      "[9a/e2401f] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 110 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [modest_elion] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[34/f8e3c8] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[34/f8e3c8] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 20 chain IDs\n",
      "Wrote output to 20 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 20 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 20 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [big_varahamihira] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 9\n",
      "\n",
      "executor >  local (1)\n",
      "[01/64447a] process > execute_jobs (1) [  0%] 0 of 20\n",
      "\n",
      "executor >  local (1)\n",
      "[01/64447a] process > execute_jobs (1) [  0%] 0 of 20\n",
      "\n",
      "executor >  local (2)\n",
      "[01/64447a] process > execute_jobs (1) [  5%] 1 of 20\n",
      "\n",
      "executor >  local (2)\n",
      "[01/64447a] process > execute_jobs (1) [  5%] 1 of 20\n",
      "\n",
      "executor >  local (3)\n",
      "[c7/5a9052] process > execute_jobs (3) [ 10%] 2 of 20\n",
      "\n",
      "executor >  local (3)\n",
      "[c7/5a9052] process > execute_jobs (3) [ 10%] 2 of 20\n",
      "\n",
      "executor >  local (4)\n",
      "[2d/59845d] process > execute_jobs (4) [ 15%] 3 of 20\n",
      "\n",
      "executor >  local (4)\n",
      "[2d/59845d] process > execute_jobs (4) [ 15%] 3 of 20\n",
      "\n",
      "executor >  local (5)\n",
      "[55/03fb2e] process > execute_jobs (5) [ 20%] 4 of 20\n",
      "\n",
      "executor >  local (5)\n",
      "[55/03fb2e] process > execute_jobs (5) [ 20%] 4 of 20\n",
      "\n",
      "executor >  local (6)\n",
      "[ed/96daa4] process > execute_jobs (6) [ 25%] 5 of 20\n",
      "\n",
      "executor >  local (7)\n",
      "[ea/1ad4a3] process > execute_jobs (7) [ 30%] 6 of 20\n",
      "\n",
      "executor >  local (8)\n",
      "[ab/318280] process > execute_jobs (8) [ 35%] 7 of 20\n",
      "\n",
      "executor >  local (9)\n",
      "[c9/58ddad] process > execute_jobs (9) [ 40%] 8 of 20\n",
      "\n",
      "executor >  local (10)\n",
      "[17/42994c] process > execute_jobs (11) [ 45%] 9 of 20\n",
      "\n",
      "executor >  local (11)\n",
      "[84/570420] process > execute_jobs (10) [ 50%] 10 of 20\n",
      "\n",
      "executor >  local (12)\n",
      "[28/211b96] process > execute_jobs (12) [ 55%] 11 of 20\n",
      "\n",
      "executor >  local (13)\n",
      "[d7/72acab] process > execute_jobs (15) [ 60%] 12 of 20\n",
      "\n",
      "executor >  local (13)\n",
      "[d7/72acab] process > execute_jobs (15) [ 60%] 12 of 20\n",
      "\n",
      "executor >  local (14)\n",
      "[94/be45f8] process > execute_jobs (14) [ 65%] 13 of 20\n",
      "\n",
      "executor >  local (14)\n",
      "[94/be45f8] process > execute_jobs (14) [ 65%] 13 of 20\n",
      "\n",
      "executor >  local (15)\n",
      "[a5/1f3a34] process > execute_jobs (13) [ 70%] 14 of 20\n",
      "\n",
      "executor >  local (15)\n",
      "[a5/1f3a34] process > execute_jobs (13) [ 70%] 14 of 20\n",
      "\n",
      "executor >  local (16)\n",
      "[7f/c90500] process > execute_jobs (16) [ 75%] 15 of 20\n",
      "\n",
      "executor >  local (17)\n",
      "[eb/0e9154] process > execute_jobs (19) [ 80%] 16 of 20\n",
      "\n",
      "executor >  local (18)\n",
      "[7f/582c22] process > execute_jobs (18) [ 85%] 17 of 20\n",
      "\n",
      "executor >  local (19)\n",
      "[30/fa7b90] process > execute_jobs (17) [ 90%] 18 of 20\n",
      "\n",
      "executor >  local (20)\n",
      "[9c/0c9c33] process > execute_jobs (20) [ 95%] 19 of 20\n",
      "\n",
      "executor >  local (20)\n",
      "[9c/0c9c33] process > execute_jobs (20) [100%] 20 of 20 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:33.010925\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 9 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_34.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.072742\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712569786\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712569786\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:24.904273\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 1\n",
      "* paralogs: 0\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 1 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 1 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 8 chains\n",
      "stitch fragments: processing 1 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_34.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_34.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: ORTH\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 0 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.157501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: CAVNZK010000320\t100001\t126840\tPeriod.1\t1000\t-\t100001\t126840\t0,0,0\t27\t98,167,101,107,152,137,237,107,112,135,177,140,134,163,149,122,102,238,215,75,134,44,94,154,83,111,21,\t0,1470,2463,2791,3805,4585,5187,5547,5790,6648,7230,8717,10060,10280,11680,13452,14301,14810,15632,16252,18787,19160,19642,21290,24582,25077,26818,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 0 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:36:25.232095\n",
      "gene_loss_summary: 0 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as L: % intact 0.11013215859030837 < 0.2\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class L assigned color in the bed file: 255,50,50\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as L :: child projections classes: ['L']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.059004\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 0 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'PI', 'UL', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:57.068518\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:36:27.984263\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [naughty_marconi] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[1d/56ede4] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[1d/56ede4] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 114 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [sleepy_kalam] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[3e/6f1acc] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[3e/6f1acc] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 29 chain IDs\n",
      "Wrote output to 29 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 29 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 29 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [goofy_lamport] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 4\n",
      "\n",
      "executor >  local (1)\n",
      "[48/d10146] process > execute_jobs (1) [  0%] 0 of 19\n",
      "\n",
      "executor >  local (1)\n",
      "[48/d10146] process > execute_jobs (1) [  0%] 0 of 29\n",
      "\n",
      "executor >  local (2)\n",
      "[90/557d94] process > execute_jobs (3) [  0%] 0 of 29\n",
      "\n",
      "executor >  local (2)\n",
      "[48/d10146] process > execute_jobs (1) [  3%] 1 of 29\n",
      "\n",
      "executor >  local (2)\n",
      "[90/557d94] process > execute_jobs (3) [  6%] 2 of 29\n",
      "\n",
      "executor >  local (3)\n",
      "[9a/f4781c] process > execute_jobs (4) [  6%] 2 of 29\n",
      "\n",
      "executor >  local (3)\n",
      "[9a/f4781c] process > execute_jobs (4) [ 10%] 3 of 29\n",
      "\n",
      "executor >  local (4)\n",
      "[1d/0fa8e8] process > execute_jobs (2) [ 10%] 3 of 29\n",
      "\n",
      "executor >  local (5)\n",
      "[e1/307324] process > execute_jobs (5) [ 13%] 4 of 29\n",
      "\n",
      "executor >  local (5)\n",
      "[e1/307324] process > execute_jobs (5) [ 17%] 5 of 29\n",
      "\n",
      "executor >  local (6)\n",
      "[29/a5643a] process > execute_jobs (8) [ 17%] 5 of 29\n",
      "\n",
      "executor >  local (6)\n",
      "[29/a5643a] process > execute_jobs (8) [ 20%] 6 of 29\n",
      "\n",
      "executor >  local (7)\n",
      "[26/722e9b] process > execute_jobs (6) [ 20%] 6 of 29\n",
      "\n",
      "executor >  local (7)\n",
      "[26/722e9b] process > execute_jobs (6) [ 24%] 7 of 29\n",
      "\n",
      "executor >  local (8)\n",
      "[9f/52c462] process > execute_jobs (7) [ 24%] 7 of 29\n",
      "\n",
      "executor >  local (9)\n",
      "[03/33da2e] process > execute_jobs (9) [ 27%] 8 of 29\n",
      "\n",
      "executor >  local (9)\n",
      "[03/33da2e] process > execute_jobs (9) [ 27%] 8 of 29\n",
      "\n",
      "executor >  local (9)\n",
      "[03/33da2e] process > execute_jobs (9) [ 31%] 9 of 29\n",
      "\n",
      "executor >  local (10)\n",
      "[9f/319f3d] process > execute_jobs (11) [ 31%] 9 of 29\n",
      "\n",
      "executor >  local (11)\n",
      "[c1/258a36] process > execute_jobs (12) [ 34%] 10 of 29\n",
      "\n",
      "executor >  local (11)\n",
      "[c1/258a36] process > execute_jobs (12) [ 34%] 10 of 29\n",
      "\n",
      "executor >  local (12)\n",
      "[cf/076833] process > execute_jobs (10) [ 37%] 11 of 29\n",
      "\n",
      "executor >  local (12)\n",
      "[cf/076833] process > execute_jobs (10) [ 37%] 11 of 29\n",
      "\n",
      "executor >  local (13)\n",
      "[42/f7cf64] process > execute_jobs (13) [ 41%] 12 of 29\n",
      "\n",
      "executor >  local (13)\n",
      "[42/f7cf64] process > execute_jobs (13) [ 41%] 12 of 29\n",
      "\n",
      "executor >  local (14)\n",
      "[53/d6146d] process > execute_jobs (14) [ 44%] 13 of 29\n",
      "\n",
      "executor >  local (15)\n",
      "[54/016d3b] process > execute_jobs (15) [ 48%] 14 of 29\n",
      "\n",
      "executor >  local (16)\n",
      "[e2/ce742f] process > execute_jobs (16) [ 51%] 15 of 29\n",
      "\n",
      "executor >  local (16)\n",
      "[e2/ce742f] process > execute_jobs (16) [ 51%] 15 of 29\n",
      "\n",
      "executor >  local (17)\n",
      "[65/04c1a7] process > execute_jobs (17) [ 55%] 16 of 29\n",
      "\n",
      "executor >  local (18)\n",
      "[b1/0540e6] process > execute_jobs (18) [ 58%] 17 of 29\n",
      "\n",
      "executor >  local (19)\n",
      "[82/dbeeb6] process > execute_jobs (19) [ 62%] 18 of 29\n",
      "\n",
      "executor >  local (19)\n",
      "[82/dbeeb6] process > execute_jobs (19) [ 62%] 18 of 29\n",
      "\n",
      "executor >  local (20)\n",
      "[31/630d25] process > execute_jobs (22) [ 65%] 19 of 29\n",
      "\n",
      "executor >  local (21)\n",
      "[0c/9ef74b] process > execute_jobs (20) [ 68%] 20 of 29\n",
      "\n",
      "executor >  local (22)\n",
      "[73/09b845] process > execute_jobs (21) [ 72%] 21 of 29\n",
      "\n",
      "executor >  local (22)\n",
      "[73/09b845] process > execute_jobs (21) [ 72%] 21 of 29\n",
      "\n",
      "executor >  local (22)\n",
      "[73/09b845] process > execute_jobs (21) [ 75%] 22 of 29\n",
      "\n",
      "executor >  local (23)\n",
      "[15/53c6b8] process > execute_jobs (23) [ 75%] 22 of 29\n",
      "\n",
      "executor >  local (24)\n",
      "[e0/0def06] process > execute_jobs (24) [ 79%] 23 of 29\n",
      "\n",
      "executor >  local (25)\n",
      "[5e/f0ff03] process > execute_jobs (25) [ 82%] 24 of 29\n",
      "\n",
      "executor >  local (26)\n",
      "[6b/1c3128] process > execute_jobs (26) [ 86%] 25 of 29\n",
      "\n",
      "executor >  local (26)\n",
      "[6b/1c3128] process > execute_jobs (26) [ 86%] 25 of 29\n",
      "\n",
      "executor >  local (27)\n",
      "[e9/54fb20] process > execute_jobs (29) [ 89%] 26 of 29\n",
      "\n",
      "executor >  local (27)\n",
      "[e9/54fb20] process > execute_jobs (29) [ 89%] 26 of 29\n",
      "\n",
      "executor >  local (27)\n",
      "[e9/54fb20] process > execute_jobs (29) [ 93%] 27 of 29\n",
      "\n",
      "executor >  local (28)\n",
      "[1c/58f536] process > execute_jobs (27) [ 93%] 27 of 29\n",
      "\n",
      "executor >  local (29)\n",
      "[ab/86ffbb] process > execute_jobs (28) [ 96%] 28 of 29\n",
      "\n",
      "executor >  local (29)\n",
      "[ab/86ffbb] process > execute_jobs (28) [ 96%] 28 of 29\n",
      "\n",
      "executor >  local (29)\n",
      "[ab/86ffbb] process > execute_jobs (28) [100%] 29 of 29 ✔\n",
      "\n",
      "executor >  local (29)\n",
      "[ab/86ffbb] process > execute_jobs (28) [100%] 29 of 29 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:39.262347\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 10 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_37.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 2 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 2 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 2 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.113830\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712569948\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712569948\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 2 result files to combine\n",
      "merge_chains_output: got 2 keys in chain_genes_data\n",
      "merge_chains_output: got 2 keys in chain_raw_data\n",
      "merge_chains_output: There were 2 transcript lines and 2 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:25.398959\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 2\n",
      "classify_chains: total number of transcripts: 2\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 1\n",
      "* paralogs: 0\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 1 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 1 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 9 chains\n",
      "stitch fragments: processing 1 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_37.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_37.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: ORTH\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 0 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.136824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: NC_062259\t98336\t127381\tPeriod.1\t1000\t+\t98336\t127381\t0,0,0\t27\t21,111,83,154,91,65,131,75,215,238,102,122,149,149,134,143,177,144,112,116,238,179,158,152,113,167,98,\t0,1638,2283,4638,5975,6449,6764,9600,10088,11560,12241,13046,14145,15132,16060,16702,17551,18040,18547,18785,19015,22298,23043,24522,26252,27523,28947,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 0 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:39:06.929426\n",
      "gene_loss_summary: 0 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as L: % intact 0.11013215859030837 < 0.2\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class L assigned color in the bed file: 255,50,50\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as L :: child projections classes: ['L']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.060969\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 0 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'UL', 'PI', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:57.496717\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:39:09.827899\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [focused_majorana] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[03/05b7fa] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[03/05b7fa] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[03/05b7fa] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 94 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [irreverent_lamarr] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[58/d2b336] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[58/d2b336] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[58/d2b336] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 15 chain IDs\n",
      "Wrote output to 15 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 15 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 15 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [ecstatic_meucci] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 6\n",
      "\n",
      "executor >  local (1)\n",
      "[cd/f9aa46] process > execute_jobs (4) [  0%] 0 of 15\n",
      "\n",
      "executor >  local (2)\n",
      "[cd/f9aa46] process > execute_jobs (4) [  6%] 1 of 15\n",
      "\n",
      "executor >  local (3)\n",
      "[0c/9ad7e1] process > execute_jobs (3) [ 13%] 2 of 15\n",
      "\n",
      "executor >  local (4)\n",
      "[bd/16ba27] process > execute_jobs (1) [ 20%] 3 of 15\n",
      "\n",
      "executor >  local (5)\n",
      "[db/da8e3f] process > execute_jobs (6) [ 26%] 4 of 15\n",
      "\n",
      "executor >  local (6)\n",
      "[48/2e55af] process > execute_jobs (5) [ 33%] 5 of 15\n",
      "\n",
      "executor >  local (6)\n",
      "[48/2e55af] process > execute_jobs (5) [ 40%] 6 of 15\n",
      "\n",
      "executor >  local (7)\n",
      "[3f/40c4bf] process > execute_jobs (8) [ 40%] 6 of 15\n",
      "\n",
      "executor >  local (7)\n",
      "[3f/40c4bf] process > execute_jobs (8) [ 46%] 7 of 15\n",
      "\n",
      "executor >  local (8)\n",
      "[cd/d5a1d4] process > execute_jobs (7) [ 46%] 7 of 15\n",
      "\n",
      "executor >  local (8)\n",
      "[cd/d5a1d4] process > execute_jobs (7) [ 53%] 8 of 15\n",
      "\n",
      "executor >  local (9)\n",
      "[3f/04e1ee] process > execute_jobs (10) [ 53%] 8 of 15\n",
      "\n",
      "executor >  local (10)\n",
      "[5e/54405c] process > execute_jobs (9)  [ 60%] 9 of 15\n",
      "\n",
      "executor >  local (10)\n",
      "[5e/54405c] process > execute_jobs (9)  [ 60%] 9 of 15\n",
      "\n",
      "executor >  local (11)\n",
      "[92/ce7e2b] process > execute_jobs (12) [ 66%] 10 of 15\n",
      "\n",
      "executor >  local (11)\n",
      "[92/ce7e2b] process > execute_jobs (12) [ 66%] 10 of 15\n",
      "\n",
      "executor >  local (12)\n",
      "[db/8b4910] process > execute_jobs (13) [ 73%] 11 of 15\n",
      "\n",
      "executor >  local (13)\n",
      "[34/20068f] process > execute_jobs (11) [ 80%] 12 of 15\n",
      "\n",
      "executor >  local (13)\n",
      "[34/20068f] process > execute_jobs (11) [ 80%] 12 of 15\n",
      "\n",
      "executor >  local (14)\n",
      "[10/0a9874] process > execute_jobs (15) [ 86%] 13 of 15\n",
      "\n",
      "executor >  local (15)\n",
      "[67/cd051d] process > execute_jobs (14) [ 93%] 14 of 15\n",
      "\n",
      "executor >  local (15)\n",
      "[67/cd051d] process > execute_jobs (14) [100%] 15 of 15 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:30.255721\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 5 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_39.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.090860\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712570099\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712570099\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:23.610382\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 1\n",
      "* paralogs: 0\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 1 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 1 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 4 chains\n",
      "stitch fragments: processing 1 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_39.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_39.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: ORTH\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 0 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.186013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: NC_059534\t98307\t123677\tPeriod.1\t1000\t+\t98307\t123677\t0,0,0\t27\t21,111,83,169,97,47,131,75,224,223,125,119,149,133,107,140,177,135,97,113,253,137,158,98,101,167,98,\t0,1667,2803,5840,7241,7545,7939,9724,10118,10556,11175,11659,13767,14332,14547,15503,16189,18232,18697,18921,19149,21991,22671,23503,23836,24337,25272,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 0 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:41:37.595851\n",
      "gene_loss_summary: 0 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as L: % intact 0.1412180052956752 < 0.2\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class L assigned color in the bed file: 255,50,50\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as L :: child projections classes: ['L']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.061750\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 0 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'I', 'PI', 'UL'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:55.650909\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:41:40.342657\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [sharp_booth] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[9e/e9ce61] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[9e/e9ce61] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[9e/e9ce61] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 41 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [intergalactic_thompson] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[a7/f5fb2b] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[a7/f5fb2b] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 2 chain IDs\n",
      "Wrote output to 2 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 2 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 2 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [furious_boltzmann] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 2\n",
      "\n",
      "executor >  local (1)\n",
      "[23/336f55] process > execute_jobs (2) [  0%] 0 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[23/336f55] process > execute_jobs (2) [ 50%] 1 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[71/66b1b1] process > execute_jobs (1) [100%] 2 of 2 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:22.289539\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_42.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.068836\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712570240\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712570240\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:21.578124\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_42.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_42.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.176403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: FR989951\t100007\t130319\tPeriod.1\t1000\t-\t100007\t130319\t0,0,0\t14\t95,101,183,140,86,174,127,114,207,175,75,131,89,79,\t0,1630,3576,5797,11446,14437,17458,19376,19991,20724,23558,24365,29389,30233,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 13 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:43:58.228064\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.066662\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'I', 'PI', 'UL'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:53.690180\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:44:00.998044\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [boring_kowalevski] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[24/641d06] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[24/641d06] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 36 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [ridiculous_blackwell] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[6f/0cbcbf] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[6f/0cbcbf] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n",
      "executor >  local (1)\n",
      "[6f/0cbcbf] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 1 chain IDs\n",
      "Wrote output to 1 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 1 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 1 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [kickass_shockley] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[12/9919d2] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[12/9919d2] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:21.677827\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_44.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.091131\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712570381\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712570381\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:22.187252\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_44.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_44.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.158709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: OU538732\t100055\t118860\tPeriod.1\t1000\t-\t100055\t118860\t0,0,0\t12\t86,101,177,127,111,113,111,220,137,134,68,124,\t0,1914,7371,8875,9589,10649,11080,11672,12456,13930,18432,18681,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 15 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:46:18.905304\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.067225\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'I', 'PI', 'UL'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:54.260036\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:46:21.463071\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [special_carlsson] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[b1/232245] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[b1/232245] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[b1/232245] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 39 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [nauseous_mclean] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[d0/3dfe26] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[d0/3dfe26] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 2 chain IDs\n",
      "Wrote output to 2 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 2 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 2 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [goofy_kalam] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[8f/72083a] process > execute_jobs (1) [  0%] 0 of 2\n",
      "\n",
      "executor >  local (1)\n",
      "[8f/72083a] process > execute_jobs (1) [  0%] 0 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[8f/72083a] process > execute_jobs (1) [ 50%] 1 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[8f/72083a] process > execute_jobs (1) [ 50%] 1 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[e9/83ca0f] process > execute_jobs (2) [100%] 2 of 2 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:22.635949\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_46.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.098854\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712570536\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712570536\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:35.771255\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_46.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_46.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.185889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: OX637275\t100001\t115561\tPeriod.1\t1000\t-\t100001\t115561\t0,0,0\t15\t108,101,77,213,134,165,124,101,98,120,163,84,113,98,94,\t0,1439,2022,2269,3003,5975,8680,9451,10052,10302,10723,12040,12711,14438,15466,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 12 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:47:54.079789\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.871394\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'I', 'PI', 'UL'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:09.330882\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:47:59.044117\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [sad_raman] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[65/f75e10] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[65/f75e10] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 40 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [fervent_wright] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[60/704cbe] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[60/704cbe] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 1 chain IDs\n",
      "Wrote output to 1 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 1 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 1 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [prickly_engelbart] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[9b/b87f8e] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[9b/b87f8e] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[9b/b87f8e] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:21.756121\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_48.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.098196\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712570628\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712570628\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:31.633230\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_48.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_48.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.185893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: NC_059680\t100000\t127512\tPeriod.1\t1000\t+\t100000\t127512\t0,0,0\t15\t91,59,122,62,258,129,122,137,136,204,155,190,101,26,99,\t0,647,5159,7289,7959,8642,9414,12539,13382,15499,21084,21747,23041,25774,27413,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 12 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:50:26.439813\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.068124\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'UL', 'PI', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:02:03.681420\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:50:29.414302\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [modest_albattani] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[2d/cf97c6] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[2d/cf97c6] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n",
      "executor >  local (1)\n",
      "[2d/cf97c6] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 36 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [mighty_shannon] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[aa/86f8d2] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[aa/86f8d2] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 1 chain IDs\n",
      "Wrote output to 1 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 1 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 1 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [backstabbing_faggin] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[2c/ce08cc] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[2c/ce08cc] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[2c/ce08cc] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n",
      "executor >  local (1)\n",
      "[2c/ce08cc] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:23.295119\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_50.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.066142\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712570776\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712570776\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:28.426776\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_50.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_50.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.199142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: CM054800\t100000\t124499\tPeriod.1\t1000\t+\t100000\t124499\t0,0,0\t13\t88,74,113,211,111,113,115,133,157,155,169,104,29,\t0,1110,8469,11846,12599,12957,15634,16142,17405,21775,22431,23861,24470,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 14 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:52:55.613677\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.068464\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'PI', 'I', 'UL'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:02:00.920993\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:52:58.437050\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [jolly_cray] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[b9/a3d16a] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[b9/a3d16a] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 41 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [astonishing_rubens] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[e2/f23cb7] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[e2/f23cb7] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 3 chain IDs\n",
      "Wrote output to 3 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 3 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 3 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [kickass_montalcini] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[f5/b6003c] process > execute_jobs (1) [  0%] 0 of 3\n",
      "\n",
      "executor >  local (2)\n",
      "[f5/b6003c] process > execute_jobs (1) [ 33%] 1 of 3\n",
      "\n",
      "executor >  local (3)\n",
      "[7f/7f530c] process > execute_jobs (3) [ 66%] 2 of 3\n",
      "\n",
      "executor >  local (3)\n",
      "[7f/7f530c] process > execute_jobs (3) [100%] 3 of 3 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:23.119025\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_53.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.088146\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712570923\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712570923\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:25.812453\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_53.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_53.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.166251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: CAVNZK010000320\t100001\t125163\tPeriod.1\t1000\t-\t100001\t125163\t0,0,0\t14\t99,29,101,190,157,133,127,122,129,238,137,122,86,88,\t0,1579,2463,3777,7250,10310,11708,13452,14274,14810,15677,18805,24582,25074,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 13 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:55:21.460244\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.066322\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'I', 'UL', 'PI'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:57.965678\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:55:24.149543\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [jolly_morse] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[78/236f1c] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[78/236f1c] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[78/236f1c] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 42 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [pedantic_austin] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[c4/d06558] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[c4/d06558] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[c4/d06558] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 3 chain IDs\n",
      "Wrote output to 3 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 3 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 3 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [tiny_engelbart] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[cd/8708f9] process > execute_jobs (2) [  0%] 0 of 3\n",
      "\n",
      "executor >  local (2)\n",
      "[cd/8708f9] process > execute_jobs (2) [ 33%] 1 of 3\n",
      "\n",
      "executor >  local (2)\n",
      "[7e/ff4fc2] process > execute_jobs (1) [ 66%] 2 of 3\n",
      "\n",
      "executor >  local (3)\n",
      "[6c/07ac8b] process > execute_jobs (3) [ 66%] 2 of 3\n",
      "\n",
      "executor >  local (3)\n",
      "[6c/07ac8b] process > execute_jobs (3) [100%] 3 of 3 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:23.277199\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_55.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.071624\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712571071\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712571071\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:27.432772\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_55.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_55.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.186340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: NC_062259\t100000\t127381\tPeriod.1\t1000\t+\t100000\t127381\t0,0,0\t15\t88,95,113,75,238,129,122,127,133,157,125,195,113,29,96,\t0,607,5100,8074,9896,10577,11382,12475,13468,15887,20688,21375,24588,25888,27285,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 12 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 15:57:48.923586\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.061705\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'UL', 'PI', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:59.732643\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 15:57:51.803746\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [hungry_wiles] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[d4/01d492] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[d4/01d492] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[d4/01d492] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 37 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [determined_hypatia] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[32/79e2af] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[32/79e2af] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[32/79e2af] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 2 chain IDs\n",
      "Wrote output to 2 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 2 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 2 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [magical_mclean] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 2\n",
      "\n",
      "executor >  local (1)\n",
      "[d0/2b3632] process > execute_jobs (1) [  0%] 0 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[5a/e658c2] process > execute_jobs (2) [  0%] 0 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[d0/2b3632] process > execute_jobs (1) [ 50%] 1 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[5a/e658c2] process > execute_jobs (2) [100%] 2 of 2 ✔\n",
      "\n",
      "executor >  local (2)\n",
      "[5a/e658c2] process > execute_jobs (2) [100%] 2 of 2 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:22.927251\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_58.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.092329\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712571213\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712571213\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:22.393427\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_58.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_15_58.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.154240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: NC_059534\t100000\t123677\tPeriod.1\t1000\t+\t100000\t123677\t0,0,0\t16\t88,59,115,75,211,129,119,103,133,186,76,116,168,101,35,99,\t0,1134,6219,8031,8875,9482,9966,12092,12639,14496,17025,20298,20968,22143,23174,23578,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 11 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 16:00:11.184052\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.061549\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'UL', 'PI', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:54.555266\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 16:00:13.987442\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [desperate_kilby] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[6e/49910f] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[6e/49910f] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[6e/49910f] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 57 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [distracted_mayer] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[bc/cbde18] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[bc/cbde18] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[bc/cbde18] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 5 chain IDs\n",
      "Wrote output to 5 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 5 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 5 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [elegant_volhard] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[bf/767719] process > execute_jobs (2) [  0%] 0 of 5\n",
      "\n",
      "executor >  local (1)\n",
      "[bf/767719] process > execute_jobs (2) [  0%] 0 of 5\n",
      "\n",
      "executor >  local (2)\n",
      "[2c/e31080] process > execute_jobs (3) [  0%] 0 of 5\n",
      "\n",
      "executor >  local (2)\n",
      "[bf/767719] process > execute_jobs (2) [ 20%] 1 of 5\n",
      "\n",
      "executor >  local (2)\n",
      "[2c/e31080] process > execute_jobs (3) [ 40%] 2 of 5\n",
      "\n",
      "executor >  local (3)\n",
      "[42/8d1ddb] process > execute_jobs (4) [ 40%] 2 of 5\n",
      "\n",
      "executor >  local (4)\n",
      "[9e/3ab59a] process > execute_jobs (1) [ 60%] 3 of 5\n",
      "\n",
      "executor >  local (4)\n",
      "[9e/3ab59a] process > execute_jobs (1) [ 60%] 3 of 5\n",
      "\n",
      "executor >  local (5)\n",
      "[52/3108e0] process > execute_jobs (5) [ 80%] 4 of 5\n",
      "\n",
      "executor >  local (5)\n",
      "[52/3108e0] process > execute_jobs (5) [100%] 5 of 5 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:24.637922\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_00.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.067474\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712571357\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712571357\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:23.116001\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_00.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_00.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.150805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: FR989951\t100025\t130319\tPeriod.1\t1000\t-\t100025\t130319\t0,0,0\t16\t77,101,101,182,140,101,189,127,109,114,168,175,75,131,89,79,\t0,1612,2274,3558,5779,11413,14404,17440,18104,19358,19964,20706,23540,24347,29371,30215,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 11 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 16:02:35.880613\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.073815\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'UL', 'PI', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:55.348113\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n",
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 16:02:38.604412\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [deadly_wiles] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[6d/f53b13] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[6d/f53b13] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 67 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [infallible_sammet] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[3c/3c5618] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[3c/3c5618] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 1 chain IDs\n",
      "Wrote output to 1 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 1 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 1 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [crazy_brenner] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[ee/02dc48] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[ee/02dc48] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:23.097197\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_03.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.075805\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712571500\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712571500\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:22.937350\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_03.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_03.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.161708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: OU538732\t100052\t118860\tPeriod.1\t1000\t-\t100052\t118860\t0,0,0\t16\t89,104,101,47,135,109,171,127,109,113,111,220,75,134,77,124,\t0,364,1917,2828,3558,4834,7380,8878,9594,10652,11083,11675,13394,13933,18426,18684,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 11 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 16:04:58.745420\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.058584\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'I', 'UL', 'PI'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:54.912672\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 16:05:01.512009\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [focused_nobel] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[2f/cf869f] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[2f/cf869f] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 65 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [fervent_meucci] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[8e/9fca03] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[8e/9fca03] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 2 chain IDs\n",
      "Wrote output to 2 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 2 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 2 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [nostalgic_gilbert] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 2\n",
      "\n",
      "executor >  local (1)\n",
      "[91/425037] process > execute_jobs (2) [  0%] 0 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[91/425037] process > execute_jobs (2) [ 50%] 1 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[46/b64779] process > execute_jobs (1) [100%] 2 of 2 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:22.916092\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_05.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.099330\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712571642\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712571642\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:22.190459\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 1\n",
      "* paralogs: 0\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 1 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 1 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 1 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_05.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_05.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: ORTH\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 0 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.153096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: OX637275\t100001\t115561\tPeriod.1\t1000\t-\t100001\t115561\t0,0,0\t15\t108,119,101,176,177,124,86,135,120,195,188,84,113,98,94,\t0,923,1439,2269,5963,8680,9466,10015,10302,10691,11475,12040,12711,14438,15466,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 12 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 16:07:20.676257\n",
      "gene_loss_summary: 0 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as L: % intact 0.17994858611825193 < 0.2\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class L assigned color in the bed file: 255,50,50\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as L :: child projections classes: ['L']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.061301\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 0 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'PI', 'UL', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:54.358758\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 16:07:23.310807\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [hungry_easley] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[d5/8f91b0] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[d5/8f91b0] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[d5/8f91b0] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 64 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [exotic_goldberg] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[55/20e876] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[55/20e876] process > execute_jobs (1) [  0%] 0 of 1 ✔\n",
      "\n",
      "executor >  local (1)\n",
      "[55/20e876] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 1 chain IDs\n",
      "Wrote output to 1 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 1 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 1 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [peaceful_edison] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[90/776eed] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[90/776eed] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[90/776eed] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:22.394727\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_07.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.082427\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712571784\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712571784\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:22.243741\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_07.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_07.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.174231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: NC_059680\t100000\t127512\tPeriod.1\t1000\t+\t100000\t127512\t0,0,0\t17\t91,89,100,97,122,91,182,129,145,134,136,204,141,140,101,104,99,\t0,617,2470,4384,5159,7023,7544,8642,9414,12539,13382,15499,21765,22664,23041,23705,27413,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 10 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 16:09:41.816799\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.054204\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'PI', 'UL', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:54.185639\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 16:09:44.496739\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [exotic_mayer] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[4e/0c8154] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[4e/0c8154] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 62 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [elegant_thompson] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[8a/251471] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[8a/251471] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[8a/251471] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 2 chain IDs\n",
      "Wrote output to 2 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 2 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 2 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [spontaneous_davinci] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 2\n",
      "\n",
      "executor >  local (1)\n",
      "[36/ba0998] process > execute_jobs (2) [  0%] 0 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[36/ba0998] process > execute_jobs (2) [ 50%] 1 of 2\n",
      "\n",
      "executor >  local (2)\n",
      "[b1/64efc9] process > execute_jobs (1) [100%] 2 of 2 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:24.369349\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_10.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.073157\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712571927\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712571927\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:21.925996\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_10.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_10.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.152349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: CM054800\t100000\t125598\tPeriod.1\t1000\t+\t100000\t125598\t0,0,0\t16\t88,86,113,111,211,111,136,115,133,204,107,171,79,104,110,47,\t0,1098,8469,10932,11846,12599,12957,15634,16142,17405,20681,22429,23231,23861,24480,25551,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 11 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 16:12:04.812381\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.065999\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'UL', 'PI', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:54.040044\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 16:12:07.953753\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [insane_keller] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[69/19a00b] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[69/19a00b] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[69/19a00b] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 58 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [goofy_rutherford] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[fb/12f047] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[fb/12f047] process > execute_jobs (1) [  0%] 0 of 1 ✔\n",
      "\n",
      "executor >  local (1)\n",
      "[fb/12f047] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 1 chain IDs\n",
      "Wrote output to 1 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 1 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 1 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [gigantic_lamport] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[63/0e0ecb] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[63/0e0ecb] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[63/0e0ecb] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:22.483437\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_12.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.089934\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712572068\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712572068\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:22.606448\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_12.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_12.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.178053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: CAVNZK010000320\t99998\t125163\tPeriod.1\t1000\t-\t99998\t125163\t0,0,0\t17\t102,104,101,59,172,137,204,133,127,122,129,238,137,91,122,86,88,\t0,1497,2466,2794,3780,4588,7206,10313,11711,13455,14277,14813,15680,16239,18808,24585,25077,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 10 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 16:14:27.034799\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.060058\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'PI', 'UL', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:54.707178\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 16:14:29.763816\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [hungry_einstein] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[a7/4e63d0] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[a7/4e63d0] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[a7/4e63d0] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 63 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [stoic_watson] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[28/c183ee] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[28/c183ee] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 3 chain IDs\n",
      "Wrote output to 3 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 3 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 3 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [high_austin] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 3\n",
      "\n",
      "executor >  local (1)\n",
      "[b8/01d67e] process > execute_jobs (1) [  0%] 0 of 3\n",
      "\n",
      "executor >  local (2)\n",
      "[b8/01d67e] process > execute_jobs (1) [ 33%] 1 of 3\n",
      "\n",
      "executor >  local (3)\n",
      "[07/3f2e05] process > execute_jobs (2) [ 66%] 2 of 3\n",
      "\n",
      "executor >  local (3)\n",
      "[07/3f2e05] process > execute_jobs (2) [ 66%] 2 of 3\n",
      "\n",
      "executor >  local (3)\n",
      "[07/3f2e05] process > execute_jobs (2) [100%] 3 of 3 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:29.053785\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_15.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.087726\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712572218\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712572218\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:22.646567\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_15.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_15.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 1.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.222459\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: NC_062259\t100000\t127381\tPeriod.1\t1000\t+\t100000\t127381\t0,0,0\t16\t88,95,113,91,182,238,129,145,127,133,204,195,104,113,101,74,\t0,607,5100,7936,8457,9896,10577,11382,12475,13468,15887,21375,22906,24588,25898,27307,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 11 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 16:16:56.805324\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.070061\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'UL', 'PI', 'I'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOGA pipeline is done in 0:01:55.010750\n",
      "# Make Lastz Chains #\n",
      "Version 2.0.8\n",
      "Commit: 187e313afc10382fe44c96e47f27c4466d63e114\n",
      "Branch: main\n",
      "\n",
      "* found run_lastz.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz.py\n",
      "* found run_lastz_intermediate_layer.py at /mnt/f/make_lastz_chains/standalone_scripts/run_lastz_intermediate_layer.py\n",
      "* found chain_gap_filler.py at /mnt/f/make_lastz_chains/standalone_scripts/chain_gap_filler.py\n",
      "* found faToTwoBit at /mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit\n",
      "* found twoBitToFa at /mnt/f/make_lastz_chains/HL_kent_binaries/twoBitToFa\n",
      "* found pslSortAcc at /mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc\n",
      "* found axtChain at /mnt/f/make_lastz_chains/HL_kent_binaries/axtChain\n",
      "* found axtToPsl at /mnt/f/make_lastz_chains/HL_kent_binaries/axtToPsl\n",
      "* found chainAntiRepeat at /mnt/f/make_lastz_chains/HL_kent_binaries/chainAntiRepeat\n",
      "* found chainMergeSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort\n",
      "* found chainCleaner at /mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner\n",
      "* found chainSort at /mnt/f/make_lastz_chains/HL_kent_binaries/chainSort\n",
      "* found chainScore at /mnt/f/make_lastz_chains/HL_kent_binaries/chainScore\n",
      "* found chainNet at /mnt/f/make_lastz_chains/HL_kent_binaries/chainNet\n",
      "* found chainFilter at /mnt/f/make_lastz_chains/HL_kent_binaries/chainFilter\n",
      "* found lastz at /home/saurav/miniconda3/envs/ncbi_datasets/bin/lastz\n",
      "* found nextflow at /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow\n",
      "All necessary executables found.\n",
      "Making chains for /mnt/f/temp_folder/query_genome.fa and /mnt/f/temp_folder/target_genome.fa files, saving results to /mnt/f/temp_folder/out\n",
      "Pipeline started at 2024-04-08 16:16:59.478831\n",
      "* Setting up genome sequences for target\n",
      "genomeID: target\n",
      "input sequence file: /mnt/f/temp_folder/query_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/target.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/query_genome.fa saved to /mnt/f/temp_folder/out/target.2bit\n",
      "For target (target) sequence file: /mnt/f/temp_folder/out/target.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* Setting up genome sequences for query\n",
      "genomeID: query\n",
      "input sequence file: /mnt/f/temp_folder/target_genome.fa\n",
      "is 2bit: False\n",
      "planned genome dir location: /mnt/f/temp_folder/out/query.2bit\n",
      "Initial fasta file /mnt/f/temp_folder/target_genome.fa saved to /mnt/f/temp_folder/out/query.2bit\n",
      "For query (query) sequence file: /mnt/f/temp_folder/out/query.2bit; chrom sizes saved to: /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "\n",
      "### Partition Step ###\n",
      "\n",
      "# Partitioning for target\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving target partitions to: /mnt/f/temp_folder/out/target_partitions.txt\n",
      "# Partitioning for query\n",
      "Saving partitions and creating 1 buckets for lastz output\n",
      "In particular, 0 partitions for bigger chromosomes\n",
      "And 1 buckets for smaller scaffolds\n",
      "Saving query partitions to: /mnt/f/temp_folder/out/query_partitions.txt\n",
      "Num. target partitions: 0\n",
      "Num. query partitions: 0\n",
      "Num. lastz jobs: 0\n",
      "\n",
      "### Lastz Alignment Step ###\n",
      "\n",
      "LASTZ: making jobs\n",
      "LASTZ: saved 1 jobs to /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_lastz_run/lastz_joblist.txt -c /mnt/f/temp_folder/out/temp_lastz_run/lastz_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [gloomy_solvay] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[5a/85e844] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[5a/85e844] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n",
      "executor >  local (1)\n",
      "[5a/85e844] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process lastz finished successfully\n",
      "Found 1 output files from the LASTZ step\n",
      "Please note that lastz_step.py does not produce output in case LASTZ could not find any alignment\n",
      "\n",
      "### Concatenating Lastz Results (Cat) Step ###\n",
      "\n",
      "Concatenating LASTZ output from 1 buckets\n",
      "* concatenated bucket bucket_ref_bulk_1 to /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Concatenated 1 files in total into 1 files\n",
      "\n",
      "### Build Chains Step ###\n",
      "\n",
      "Sorting PSL files, saving the results to /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/pslSortAcc nohead /mnt/f/temp_folder/out/temp_chain_run/sorted_psl /mnt/f/temp_folder/out/temp_kent /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/f/temp_folder/out/temp_concat_lastz_output/concat_0.psl.gz\n",
      "Processed 65 lines into 1 temp files\n",
      "writing /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "Cleaning up temp files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bundling psl files with the following arguments:\n",
      "* input_dir: /mnt/f/temp_folder/out/temp_chain_run/sorted_psl\n",
      "* chrom_sizes: /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "* output_dir: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "* max_bases: 1000000\n",
      "* warning_only: False\n",
      "* verbose: False\n",
      "Saving results to: /mnt/f/temp_folder/out/temp_chain_run/split_psl\n",
      "Bundling 1 psl files in total\n",
      "Written to /mnt/f/temp_folder/out/temp_chain_run/split_psl/bundle.0.psl\n",
      "DONE. Produced 1 files\n",
      "PSL bundle sub-step done\n",
      "Building axtChain joblist for 1 bundled psl files\n",
      "Saving 1 axtChain jobs to /mnt/f/temp_folder/out/temp_chain_run/chains_joblist\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_chain_run/chains_joblist -c /mnt/f/temp_folder/out/temp_chain_run/chain_run_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [crazy_goldberg] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "[-        ] process > execute_jobs [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[a4/d050c5] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[a4/d050c5] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process chain_run finished successfully\n",
      "Chain run output files saved to /mnt/f/temp_folder/out/temp_chain_run/chain\n",
      "\n",
      "### Merge Chains Step ###\n",
      "\n",
      "Executing the following sequence of piped commands:\n",
      "['find', '/mnt/f/temp_folder/out/temp_chain_run/chain', '-name', '*chain']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Saved merged results to: /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz\n",
      "\n",
      "### Fill Chains Step ###\n",
      "\n",
      "Preparing fill jobs\n",
      "gunzip -c /mnt/f/temp_folder/out/temp_chain_run/target.query.all.chain.gz > /mnt/f/temp_folder/out/temp_fill_chain/temp.all.chain\n",
      "Found 1 chain IDs\n",
      "Wrote output to 1 files starting with '/mnt/f/temp_folder/out/temp_fill_chain/fill_chain_chunks/infill_chain_'.\n",
      "Creating repeat filler jobs list\n",
      "fGot 1 chain files to fill\n",
      "Adding --unmask flag\n",
      "Saved 1 chain fill jobs to /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt\n",
      "Parallel manager: pushing job /home/saurav/miniconda3/envs/ncbi_datasets/bin/nextflow /mnt/f/make_lastz_chains/parallelization/execute_joblist.nf --joblist /mnt/f/temp_folder/out/temp_fill_chain/repeat_filler_joblist.txt -c /mnt/f/temp_folder/out/temp_fill_chain/fill_chain_config.nf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N E X T F L O W  ~  version 23.10.1\n",
      "Launching `/mnt/f/make_lastz_chains/parallelization/execute_joblist.nf` [loving_sax] DSL2 - revision: 0483b29723\n",
      "[-        ] process > execute_jobs -\n",
      "\n",
      "executor >  local (1)\n",
      "[3b/bc1796] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[3b/bc1796] process > execute_jobs (1) [  0%] 0 of 1\n",
      "\n",
      "executor >  local (1)\n",
      "[3b/bc1796] process > execute_jobs (1) [100%] 1 of 1 ✔\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "### Nextflow process fill_chain finished successfully\n",
      "Merging filled chains\n",
      "Executing the following sequence of commands in a pipe:\n",
      "['find', '/mnt/f/temp_folder/out/temp_fill_chain/filled_chain_files', '-type', 'f', '-name', '*.chain', '-print']\n",
      "['/mnt/f/make_lastz_chains/HL_kent_binaries/chainMergeSort', '-inputList=stdin', '-tempDir=/mnt/f/temp_folder/out/temp_kent']\n",
      "['gzip', '-c']\n",
      "\n",
      "Merging filled chains done\n",
      "Fill chains step complete\n",
      "\n",
      "### Clean Chains Step ###\n",
      "\n",
      "Chains were filled: using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as input\n",
      "Chain to be cleaned saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz\n",
      "Executing the following chain cleaner command:\n",
      "/mnt/f/make_lastz_chains/HL_kent_binaries/chainCleaner /mnt/f/temp_folder/out/temp_chain_run/target.query.before_cleaning.chain.gz /mnt/f/temp_folder/out/target.2bit /mnt/f/temp_folder/out/query.2bit /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp /mnt/f/temp_folder/out/temp_chain_run/removed_suspects.bed -linearGap=loose -tSizes=/mnt/f/temp_folder/out/target.chrom.sizes -qSizes=/mnt/f/temp_folder/out/query.chrom.sizes -LRfoldThreshold=2.5 -doPairs -LRfoldThresholdPairs=10 -maxPairDistance=10000 -maxSuspectScore=100000 -minBrokenChainScore=75000\n",
      "Not filtered by score chains temporary saved to /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain__temp\n",
      "Chain clean results saved to: /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain\n",
      "Chain clean DONE\n",
      "\n",
      "### All core pipeline steps done ###\n",
      "\n",
      "Chains were filled, using /mnt/f/temp_folder/out/temp_chain_run/target.query.filled.chain.gz as the last output file.\n",
      "Saved final chains file to /mnt/f/temp_folder/out/target.query.final.chain.gz\n",
      "Cleaning up the following directories\n",
      "x /mnt/f/temp_folder/out/temp_chain_run\n",
      "x /mnt/f/temp_folder/out/temp_concat_lastz_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_psl_output\n",
      "x /mnt/f/temp_folder/out/temp_lastz_run\n",
      "x /mnt/f/temp_folder/out/temp_fill_chain\n",
      "x /mnt/f/temp_folder/out/temp_kent\n",
      "And the following files:\n",
      "x /mnt/f/temp_folder/out/target.2bit\n",
      "x /mnt/f/temp_folder/out/query.2bit\n",
      "x /mnt/f/temp_folder/out/target_partitions.txt\n",
      "x /mnt/f/temp_folder/out/query_partitions.txt\n",
      "x /mnt/f/temp_folder/out/target.chrom.sizes\n",
      "x /mnt/f/temp_folder/out/query.chrom.sizes\n",
      "make_lastz_chains run done in 0:00:22.624142\n",
      "#### Initiating TOGA class ####\n",
      "# python interpreter path: /usr/bin/python3.11\n",
      "# python interpreter version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n",
      "Version 1.1.8.dev\n",
      "Commit: 97eb5a17ce76fccd9858b2ed738c51cd661292aa\n",
      "Branch: master\n",
      "\n",
      "# Python package versions\n",
      "* twobitreader: unknown version\n",
      "* networkx: 3.2.1\n",
      "* pandas: 2.1.2\n",
      "* numpy: 1.26.1\n",
      "* xgboost: 2.0.1\n",
      "! scikit-learn: Not installed - will try to install\n",
      "* joblib: 1.3.2\n",
      "* h5py: 3.10.0\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/configure.sh\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C code...\n",
      "XGBoost model not found\n",
      "Training...\n",
      "Model created\n",
      "No git repo detected, downloading CESAR using wget...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/mnt/f/temp_folder/train_model.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Fasta.o src/Fasta.c\n",
      "gcc -O3 -std=c11   -c -o src/State.o src/State.c\n",
      "gcc -O3 -std=c11   -c -o src/Params.o src/Params.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Params.c:10:\n",
      "src/Params.c: In function ‘Params__set_via_str’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Params.c:414:7: note: in expansion of macro ‘logv’\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |       ^~~~\n",
      "src/Params.c:414:31: note: format string is defined here\n",
      "  414 |       logv(1, \"Setting %s := %u\", string, *((size_t*) INT_DICT[i][1]));\n",
      "      |                              ~^\n",
      "      |                               |\n",
      "      |                               unsigned int\n",
      "      |                              %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Literal.o src/Literal.c\n",
      "gcc -O3 -std=c11   -c -o src/Arguments.o src/Arguments.c\n",
      "gcc -O3 -std=c11   -c -o src/Alignment.o src/Alignment.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Alignment.c:13:\n",
      "src/Alignment.c: In function ‘find_best_deletion’:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:55:7: note: in expansion of macro ‘logv’\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:55:103: note: format string is defined here\n",
      "   55 |       logv(6, \"lookup_query: %c%c%c (%c%c/%c%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n",
      "In file included from src/Alignment.c:13:\n",
      "src/Logging.h:33:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 17 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Alignment.c:78:7: note: in expansion of macro ‘logv’\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |       ^~~~\n",
      "src/Alignment.c:78:103: note: format string is defined here\n",
      "   78 |       logv(6, \"lookup_query: %c%c%c (%c%c%c/%c)\\tprob (highest): %E (%E)\\tbest deletion: '%s' at pos=%u\\n\",\n",
      "      |                                                                                                      ~^\n",
      "      |                                                                                                       |\n",
      "      |                                                                                                       unsigned int\n",
      "      |                                                                                                      %lu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Cesar.o src/Cesar.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/Cesar.c: In function ‘main’:\n",
      "src/Cesar.c:52:45: warning: ‘/extra/tables/’ directive output may be truncated writing 14 bytes into a region of size between 0 and 1023 [-Wformat-truncation=]\n",
      "   52 |   snprintf(prefix, PATH_STRING_LENGTH-1, \"%s/extra/tables/\", BaseDir);\n",
      "      |                                             ^~~~~~~~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:67:10: note: ‘__builtin___snprintf_chk’ output between 15 and 1038 bytes into a destination of size 1023\n",
      "   67 |   return __builtin___snprintf_chk (__s, __n, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   68 |        __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:66:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   66 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->acceptor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:68:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "   68 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->acceptor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:116:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  116 |       sprintf(fileInsideBinaryLocation, \"%s%s/%s\", prefix, parameters.clade, reference->donor);\n",
      "      |                                          ^~        ~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 2303 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Cesar.c:118:42: warning: ‘%s’ directive writing up to 1023 bytes into a region of size 256 [-Wformat-overflow=]\n",
      "  118 |       sprintf(pathInsideBinaryLocation, \"%s/%s\", BaseDir, reference->donor);\n",
      "      |                                          ^~      ~~~~~~~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Cesar.c:6:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 2 and 1280 bytes into a destination of size 256\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Model.o src/Model.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Model.c:14:\n",
      "src/Model.c: In function ‘multi_exon’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:243:13: note: in expansion of macro ‘logv’\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |             ^~~~\n",
      "src/Model.c:243:47: note: format string is defined here\n",
      "  243 |             logv(1, \"reference[%i]->sequence[%i-3-%i] == params->stop-codons[3*%i+%i]: %c == %c\", i,\n",
      "      |                                              ~^\n",
      "      |                                               |\n",
      "      |                                               int\n",
      "      |                                              %li\n",
      "In file included from src/Model.c:14:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 7 has type ‘size_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:370:71: note: format string is defined here\n",
      "  370 |           die(\"Invalid number of split codon nucleotides in file %s: %u\", params->fasta_file, params->split_emissions_donor);\n",
      "      |                                                                      ~^\n",
      "      |                                                                       |\n",
      "      |                                                                       unsigned int\n",
      "      |                                                                      %lu\n",
      "src/Model.c: In function ‘create_profile_chain’:\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Model.c:36:26: warning: ‘%s’ directive writing up to 19 bytes into a region of size 14 [-Wformat-overflow=]\n",
      "   36 |     sprintf(name, \"match_%s\", profile->name);\n",
      "      |                          ^~\n",
      "In file included from /usr/include/stdio.h:867,\n",
      "                 from src/Model.c:11:\n",
      "/usr/include/x86_64-linux-gnu/bits/stdio2.h:36:10: note: ‘__builtin___sprintf_chk’ output between 7 and 26 bytes into a destination of size 20\n",
      "   36 |   return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,\n",
      "      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "   37 |       __bos (__s), __fmt, __va_arg_pack ());\n",
      "      |       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Profile.o src/Profile.c\n",
      "gcc -O3 -std=c11   -c -o src/EmissionTable.o src/EmissionTable.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__read’:\n",
      "src/Logging.h:39:19: warning: format ‘%u’ expects argument of type ‘unsigned int’, but argument 9 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Logging.h:39:19: note: in definition of macro ‘die’\n",
      "   39 |   fprintf(stderr, \"\\x1B[31mCRITICAL %s:%d %s():\\t\\x1B[0m\" format \"\\n\", __FILE__, __LINE__, __func__, ##__VA_ARGS__); exit(FAIL_EXIT);\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:94:83: note: format string is defined here\n",
      "   94 |           die(\"Unsupported order of oligomers found in %s:%lu: Expected %lu, got %u (%s)\", filename, lineno+1, expected, index, token);\n",
      "      |                                                                                  ~^\n",
      "      |                                                                                   |\n",
      "      |                                                                                   unsigned int\n",
      "      |                                                                                  %lu\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/EmissionTable.c: In function ‘EmissionTable__by_literals’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 8 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:41: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                        ~^\n",
      "      |                                         |\n",
      "      |                                         int\n",
      "      |                                        %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 10 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:193:7: note: in expansion of macro ‘logv’\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:193:53: note: format string is defined here\n",
      "  193 |       logv(7, \"    by literals: qry=%s=%i x ref=%s=%i\", qry, column, ref, row);\n",
      "      |                                                    ~^\n",
      "      |                                                     |\n",
      "      |                                                     int\n",
      "      |                                                    %li\n",
      "In file included from src/EmissionTable.c:12:\n",
      "src/Logging.h:33:19: warning: format ‘%x’ expects argument of type ‘unsigned int’, but argument 7 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/EmissionTable.c:223:7: note: in expansion of macro ‘logv’\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |       ^~~~\n",
      "src/EmissionTable.c:223:26: note: format string is defined here\n",
      "  223 |       logv(7, \"Visit: %02x\", column);\n",
      "      |                       ~~~^\n",
      "      |                          |\n",
      "      |                          unsigned int\n",
      "      |                       %02lx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Matrix.o src/Matrix.c\n",
      "gcc -O3 -std=c11   -c -o src/Viterbi.o src/Viterbi.c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from src/Viterbi.c:15:\n",
      "src/Viterbi.c: In function ‘Viterbi__step’:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 11 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:45: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                            ~^\n",
      "      |                                             |\n",
      "      |                                             int\n",
      "      |                                            %li\n",
      "In file included from src/Viterbi.c:15:\n",
      "src/Logging.h:33:19: warning: format ‘%i’ expects argument of type ‘int’, but argument 13 has type ‘uint_fast16_t’ {aka ‘long unsigned int’} [-Wformat=]\n",
      "   33 |   fprintf(stderr, \"VERBOSE%i %s:%d %s():\\t\" format \"\\n\", level, __FILE__, __LINE__, __func__, ##__VA_ARGS__);\\\n",
      "      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "src/Viterbi.c:151:9: note: in expansion of macro ‘logv’\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |         ^~~~\n",
      "src/Viterbi.c:151:56: note: format string is defined here\n",
      "  151 |         logv(6, \"t=%lu\\ti=%s=\"SID\"\\tqry=%s=%i\\tref=%s=%i\\temission_logodd=%E\",\n",
      "      |                                                       ~^\n",
      "      |                                                        |\n",
      "      |                                                        int\n",
      "      |                                                       %li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc -O3 -std=c11   -c -o src/Sequence.o src/Sequence.c\n",
      "gcc -O3 -std=c11   -c -o src/HMM.o src/HMM.c\n",
      "gcc -O3 -std=c11   -c -o src/Logodd.o src/Logodd.c\n",
      "gcc -O3 -std=c11 -o CESAR src/Fasta.o src/State.o src/Params.o src/Literal.o src/Arguments.o src/Alignment.o src/Cesar.o src/Model.o src/Profile.o src/EmissionTable.o src/Matrix.o src/Viterbi.o src/Sequence.o src/HMM.o src/Logodd.o -lm -lc\n",
      "mv CESAR cesar\n",
      "Don't worry about '*** are the same file' message if you see it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: 'CESAR' and 'cesar' are the same file\n",
      "make: *** [Makefile:19: cesar] Error 1\n",
      "Command finished with exit code 0.\n",
      "Does it work?\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/modules/chain_score_filter /mnt/f/temp_folder/out/target.query.final.chain 15000 > /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "\n",
      "Command finished with exit code 0.\n",
      "Continue without isoforms file: not provided\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/query_genome.2bit\n",
      "Found 1 sequences in /mnt/f/temp_folder/target_genome.2bit\n",
      "Saving output to /mnt/f/temp_folder/toga_out\n",
      "Arguments stored in /mnt/f/temp_folder/toga_out/project_args.json\n",
      "\n",
      "\n",
      "#### STEP 0: making chain and bed file indexes\n",
      "\n",
      "Started chain indexing...\n",
      "chain_bst_index: indexing 2 chains\n",
      "chain_bst_index: Saved chain /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain index to /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "Started bed file indexing...\n",
      "bed_hdf5_index: indexed 1 transcripts\n",
      "\n",
      "\n",
      "#### STEP 1: Generate extract chain features jobs\n",
      "\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_chain_jobs.py /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_17.log --parallel_logs_dir /mnt/f/temp_folder/toga_out/temp_logs --jobs_num 100 --jobs /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs --jobs_file /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined --results_dir /mnt/f/temp_folder/toga_out/temp/chain_classification_results --rejected /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CHAIN_REJ.txt \n",
      "\n",
      "split_chain_jobs: Use bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed and chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "split_chain jobs: the run data overview is:\n",
      "\n",
      "* vv: False\n",
      "* jobs: /mnt/f/temp_folder/toga_out/temp/chain_classification_jobs\n",
      "* results_dir: /mnt/f/temp_folder/toga_out/temp/chain_classification_results\n",
      "* errors_dir: None\n",
      "* chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* index_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain_ID_position\n",
      "* job_size: None\n",
      "* jobs_num: 100\n",
      "* bed_index: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* jobs_file: /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "* ref: hg38\n",
      "* on_cluster: True\n",
      "split_chain_jobs: searching for intersections between reference transcripts and chains\n",
      "split_chain_jobs: chains-to-transcripts dict contains 1 records\n",
      "split_chain_jobs: skipped 0 transcripts that do not intersect any chain\n",
      "split_chain_jobs: preparing 1 commands\n",
      "split_chain_jobs: command size of 1 for each cluster job\n",
      "split_chain_jobs: results in 1 cluster jobs\n",
      "split_chain_jobs: estimated time: 0:00:00.079881\n",
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 2: Extract chain features: parallel step\n",
      "\n",
      "Extracting chain features, project name: chain_feats__mntftemp_foldertoga_out_at_1712572363\n",
      "Project path: /mnt/f/TOGA/nextflow_logs/chain_feats__mntftemp_foldertoga_out_at_1712572363\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/chain_class_jobs_combined\n",
      "Logs from individual chain runner jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 3: Merge step 2 output\n",
      "\n",
      "Reading /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "merge_chains_output: got data for 1 transcripts\n",
      "merge_chains_output: Loading the results...\n",
      "merge_chains_output: There are 1 result files to combine\n",
      "merge_chains_output: got 1 keys in chain_genes_data\n",
      "merge_chains_output: got 1 keys in chain_raw_data\n",
      "merge_chains_output: There were 1 transcript lines and 1 chain lines\n",
      "merge_chains_output: chain_genes_data dict reverted, there are 1 keys now\n",
      "merge_chains_output: Combining the data...\n",
      "merge_chains_output: got combined dict with 1 keys\n",
      "merge_chains_output: Writing output to /mnt/f/temp_folder/toga_out/temp/chain_results_df.tsv\n",
      "merge_chains_output: total runtime: 0:00:25.136495\n",
      "\n",
      "\n",
      "#### STEP 4: Classify chains using gradient boosting model\n",
      "\n",
      "Classifying chains\n",
      "classify_chains: loaded dataframe of size 1\n",
      "classify_chains: total number of transcripts: 1\n",
      "classify_chains: 0 rows with spanning chains\n",
      "classify_chains: filtered dataset contains 1 records\n",
      "classify_chains: omputing additional features...\n",
      "classify_chains: df for single-exon model contains 0 records\n",
      "classify_chains: df for multi-exon model contains 1 records\n",
      "classify_chains: loading models at /mnt/f/TOGA/models/se_model.dat (SE) and /mnt/f/TOGA/models/me_model.dat (ME)\n",
      "classify_chains: applying models to SE and ME datasets...\n",
      "classify_chains: applying -1.0 score to the spanning chains\n",
      "classify_chains: applying -2.0 score to the processed pseudogene alignments\n",
      "classify_chains: number of processed pseudogene alignments: 0\n",
      "classify_chains: arranging the final output\n",
      "/mnt/f/TOGA/modules/classify_chains.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  overall_result = pd.concat([df_se_result, df_me_result, spanning_chains_result])\n",
      "classify_chains: classification result stats:\n",
      "* orthologs: 0\n",
      "* paralogs: 1\n",
      "* spanning chains: 0\n",
      "* processed pseudogenes: 0\n",
      "classify_chains: using 0.5 as a threshold to separate orthologs from paralogs\n",
      "classify_chains: combining results for 1 individual transcripts\n",
      "classify_chains: saving the classification to /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "classify_chains: found no classifiable chains for 0 transcripts\n",
      "classify_chains: saving these transcripts to: /mnt/f/temp_folder/toga_out/temp/rejected/classify_chains_rejected.txt\n",
      "\n",
      "\n",
      "#### STEP 5: Generate CESAR jobs\n",
      "Detecting fragmented transcripts\n",
      "stitch_fragments: started stitching fragmented orthologous loci (if any)\n",
      "stitch fragments: processing 0 transcripts with scores >= 0.5\n",
      "stitch fragments: processing total of 0 chains with scores\n",
      "stitch fragments: parsing chain file /mnt/f/temp_folder/toga_out/temp/genome_alignment.chain to get a mapping between chain ID and coordinates in the query genome\n",
      "stitch fragments: parsed 1 chains\n",
      "stitch fragments: processing 0 transcripts\n",
      "stitch fragments: identified 0 fragmented transcripts\n",
      "Fragments data saved to /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "Setting up creating CESAR jobs\n",
      "Calling cmd:\n",
      "/mnt/f/TOGA/split_exon_realign_jobs.py /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5 /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst /mnt/f/temp_folder/query_genome.2bit /mnt/f/temp_folder/target_genome.2bit /mnt/f/temp_folder/toga_out --jobs_dir /mnt/f/temp_folder/toga_out/temp/cesar_jobs --jobs_num 500 --combined /mnt/f/temp_folder/toga_out/temp/cesar_combined --results /mnt/f/temp_folder/toga_out/temp/cesar_results --buckets 0 --mem_limit 16 --chains_limit 100 --skipped_genes /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt --rejected_log /mnt/f/temp_folder/toga_out/temp/rejected --cesar_binary /mnt/f/TOGA/CESAR2.0/cesar --paralogs_log /mnt/f/temp_folder/toga_out/temp/paralogs.txt --uhq_flank 50 --predefined_glp_class_path /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv --unprocessed_log /mnt/f/temp_folder/toga_out/temp/technical_cesar_err --log_file /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_17.log --cesar_logs_dir /mnt/f/temp_folder/toga_out/temp_logs  --mask_stops --check_loss /mnt/f/temp_folder/toga_out/temp/inact_mut_data --fragments_data /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "\n",
      "split_cesar_jobs: the arguments list is:\n",
      "* orthologs_file: /mnt/f/temp_folder/toga_out/temp/trans_to_chain_classes.tsv\n",
      "* bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* bdb_bed_file: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.hdf5\n",
      "* bdb_chain_file: /mnt/f/temp_folder/toga_out/temp/genome_alignment.bst\n",
      "* tDB: /mnt/f/temp_folder/query_genome.2bit\n",
      "* qDB: /mnt/f/temp_folder/target_genome.2bit\n",
      "* toga_out_dir: /mnt/f/temp_folder/toga_out\n",
      "* cesar_binary: /mnt/f/TOGA/CESAR2.0/cesar\n",
      "* jobs_num: 500\n",
      "* buckets: 0\n",
      "* mask_stops: True\n",
      "* chains_limit: 100\n",
      "* skipped_genes: /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "* mem_limit: 16.0\n",
      "* jobs_dir: /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "* combined: /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "* results: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* check_loss: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* u12: None\n",
      "* rejected_log: /mnt/f/temp_folder/toga_out/temp/rejected\n",
      "* paralogs_log: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* uhq_flank: 50\n",
      "* o2o_only: False\n",
      "* no_fpi: False\n",
      "* annotate_paralogs: False\n",
      "* fragments_data: /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "* predefined_glp_class_path: /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "* unprocessed_log: /mnt/f/temp_folder/toga_out/temp/technical_cesar_err\n",
      "* cesar_logs_dir: /mnt/f/temp_folder/toga_out/temp_logs\n",
      "* debug: False\n",
      "* mask_all_first_10p: False\n",
      "* log_file: /mnt/f/temp_folder/toga_out/toga_2024_04_08_at_16_17.log\n",
      "* quiet: False\n",
      "split_cesar_jobs: reading U12 data from None\n",
      "split_cesar_jobs: not U12 file provided: skip\n",
      "split_cesar_jobs: reading orthology data...\n",
      "split_cesar_jobs: for each transcript, find chains to produce annotations\n",
      "* selected chain class to annotate transcript Period: PARA\n",
      "split_cesar_jobs: number of transcripts to create CESAR jobs: 1\n",
      "split_cesar_jobs: total number of 1 transcript/chain pairs\n",
      "split_cesar_jobs: skipped total of 0 transcripts\n",
      "split_cesar_jobs: out of them, transcripts not intersected by chains: 0\n",
      "split_cesar_jobs: assigning MISSING class to 0 transcripts not intersected by any chain\n",
      "split_cesar_jobs: creating a list of RAM-limit buckets based on user arguments\n",
      "split_cesar_jobs: split into buckets is not required, using only the limit 16.0\n",
      "split_cesar_jobs: reading bed file /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "split_cesar_jobs: got data for 1 transcripts\n",
      "split_cesar_jobs: reading transcript fragments data from /mnt/f/temp_folder/toga_out/temp/gene_fragments.txt\n",
      "split_cesar_jobs: got data for 0 transcripts potentially fragmented in the query genome\n",
      "split_cesar_jobs: precomputing query regions for each transcript/chain pair\n",
      "split_cesar_jobs: batch size: 1\n",
      "split_cesar_jobs: first, invert gene-to-chains dict to chain-to-genes\n",
      "split_cesar_jobs: for each of 1 involved chains, precompute regions\n",
      "split_cesar_jobs: precomputed regions for 1 transcripts\n",
      "split_cesar_jobs: skipped 0 projections\n",
      "split_cesar_jobs: predefined classification for 0 projections\n",
      "split_cesar_jobs: building commands for 1 transcripts\n",
      "split_cesar_jobs: some transcripts can be omitted (see above)\n",
      " * added job for transcript Period, chains: dict_keys(['1']), memory_requirements: 2.25, u12_data: None\n",
      "split_cesar_jobs: created 1 jobs in total\n",
      "split_cesar_jobs: filling the following RAM limit buckets: [0]\n",
      "No buckets to split, saving 1 jobs into the same queue\n",
      "split_cesar_jobs: defining number of cluster jobs for each bucket\n",
      "split_cesar_jobs: based on memory, the estimated runtime proportions are:\n",
      "* bucket 0Gb: 1.0\n",
      "Final numbers of cluster jobs per bucket are:\n",
      " * bucket 0Gb: 500 jobs\n",
      "split_cesar_jobs: saving CESAR job queues to /mnt/f/temp_folder/toga_out/temp/cesar_jobs\n",
      "# split_cesar_jobs: saved part  of bucket 0 to /mnt/f/temp_folder/toga_out/temp/cesar_jobs/cesar_job_1_0 with 1 commands\n",
      "split_cesar_jobs: saving combined CESAR jobs to /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "split_cesar_jobs: saving 0 skipped transcripts to /mnt/f/temp_folder/toga_out/temp/rejected/SPLIT_CESAR.txt\n",
      "split_cesar_jobs: precomputed gene loss classes for 0 items are saved to /mnt/f/temp_folder/toga_out/temp/predefined_glp_cesar_split.tsv\n",
      "split_cesar_jobs: potentially, for some transcripts, no orthologous chains found\n",
      "split_cesar_jobs: TOGA will create 1 paralogous projections (PG class); their IDs are saved to /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "split_cesar_jobs: splitting jobs done in 0:00:00.159121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command finished with exit code 0.\n",
      "\n",
      "\n",
      "#### STEP 6: Create processed pseudogenes track\n",
      "\n",
      "Creating processed pseudogenes track.\n",
      "make_pr_pseudogenes_anno: loading chain index...\n",
      "make_pr_pseudogenes anno: 0 transcripts have processed pseudogenes\n",
      "make_pr_pseudogenes_anno: no processed pseudogenes found, skip\n",
      "\n",
      "\n",
      "### STEP 7: Execute CESAR jobs: parallel step\n",
      "\n",
      "Pushing 1 CESAR job lists\n",
      "Pushing memory bucket 0Gb to the executor\n",
      "Selected parallelization strategy: nextflow\n",
      "Parallel manager: pushing job nextflow /mnt/f/TOGA/execute_joblist.nf --joblist /mnt/f/temp_folder/toga_out/temp/cesar_combined\n",
      "## Stated polling cluster jobs until they done\n",
      "Polling iteration 0; already waiting 0 seconds.\n",
      "### CESAR jobs done ###\n",
      "\n",
      "Checking whether all CESAR results are complete\n",
      "No CESAR jobs crashed\n",
      "Logs from individual CESAR jobs are show below\n",
      "\n",
      "\n",
      "#### STEP 8: Merge STEP 7 output\n",
      "\n",
      "Merging CESAR output to make fasta and bed files.\n",
      "merge_cesar_jobs: module called with arguments:\n",
      "* input_dir: /mnt/f/temp_folder/toga_out/temp/cesar_results\n",
      "* output_bed: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* output_fasta: /mnt/f/temp_folder/toga_out/nucleotide.fasta\n",
      "* meta_data_arg: /mnt/f/temp_folder/toga_out/temp/exons_meta_data.tsv\n",
      "* skipped_arg: /mnt/f/temp_folder/toga_out/temp/rejected/CESAR_MERGE.txt\n",
      "* prot_arg: /mnt/f/temp_folder/toga_out/prot.fasta\n",
      "* codon_arg: /mnt/f/temp_folder/toga_out/codon.fasta\n",
      "* output_trash: /mnt/f/temp_folder/toga_out/temp/trash_exons.bed\n",
      "* fragm_data: /mnt/f/temp_folder/toga_out/temp/bed_fragments_to_exons.tsv\n",
      "* exclude: None\n",
      "merge_cesar_jobs: merging CESAR results from 1 output files\n",
      " * processing file cesar_job_1_0.txt 1/1\n",
      "merge_cesar_jobs: parsing file /mnt/f/temp_folder/toga_out/temp/cesar_results/cesar_job_1_0.txt with 1 reference transcript(s)\n",
      "merge_cesar_jobs: Added raw bed line for Period.1: NC_059534\t100000\t122805\tPeriod.1\t1000\t+\t100000\t122805\t0,0,0\t15\t88,86,115,88,211,129,91,115,133,186,94,146,98,101,122,\t0,1107,6219,8031,8875,9482,9966,12080,12639,14496,20160,20990,21810,22143,22683,\n",
      "merge_cesar_jobs: arranging fasta file\n",
      "merge_cesar_jobs: added 12 exons that are actually deleted or missing but annotated by CESAR\n",
      "merge_cesar_jobs: saving 1 bed lines from this part\n",
      "merge_cesar_jobs: Saving the output\n",
      "merge_cesar_jobs: writing 1 bed records to /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "CESAR results merged\n",
      "\n",
      "\n",
      "#### STEP 9: Gene loss pipeline classification\n",
      "\n",
      "Calling gene loss summary\n",
      "Classification for 0 query transcripts was already computed\n",
      "Added 0 query transcripts classified as missing\n",
      "gene_loss_summary: called module with the following arguments:\n",
      "* loss_data_arg: /mnt/f/temp_folder/toga_out/temp/inact_mut_data\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* pre_final_bed_arg: /mnt/f/temp_folder/toga_out/temp/intermediate.bed\n",
      "* bed_out: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* summary_arg: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* trace_arg: None\n",
      "* iforms_file: None\n",
      "* paral: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* exclude_arg: None\n",
      "* predefined_class: []\n",
      "* t0: 2024-04-08 16:19:21.245536\n",
      "gene_loss_summary: 1 projections are annotated as paralogs\n",
      "gene_loss_summary: 0 projections are excluded from classification\n",
      "gene_loss_summary: extracted length data for 1 reference exons\n",
      "gene_loss_summary: reading inactivating mutations data...\n",
      "* reading data from cesar_job_1_0.inact_mut.txt...\n",
      "gene_loss_summary inactivating mutations output sizes:\n",
      "* projection_to_mutations: 1\n",
      "* projection_to_p_intact_M_ignore: 1\n",
      "* projection_to_p_intact_M_intact: 1\n",
      "* projection_to_i_codon_prop: 1\n",
      "* proj_to_prop_oub: 1\n",
      "* proj_to_80_p_intact: 1\n",
      "* proj_to_80_p_present: 1\n",
      "gene_loss_summary: loaded predefined classifications for 0 query projections and 0 reference transcripts\n",
      "gene_loss_summary: in total, 1 query projections are to be classified\n",
      "gene_loss_summary: classifying query projections: decision tree part\n",
      "* Period.1 classified as PG: was present in the paralogs list\n",
      "gene_loss_summary: Assigning colors to the bed file\n",
      "* Period.1 class PG assigned color in the bed file: 159,129,112\n",
      "gene_loss_summary: assigning classes to reference transcripts based on the classifications of child query transcripts (projections)\n",
      "* transcript: Period classified as PG :: child projections classes: ['PG']\n",
      "gene_loss_summary: classifying reference genes based on transcript classifications\n",
      "gene_loss_summary: no isoforms data provided: treating reference transcripts as individual genes\n",
      "gene_loss_summary: writing summary to /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "Elapsed: 0:00:00.064219\n",
      "\n",
      "\n",
      "#### STEP 10: Create orthology relationships table\n",
      "\n",
      "make_query_isoforms: inferring genes from annotated isoforms in the query\n",
      "make_query_isoforms: called with the following arguments:\n",
      "* query_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* query_isoforms: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* save_genes_track: /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "* ignore_color: False\n",
      "* gene_prefix: TOGA\n",
      "make_query_isoforms: reading query annotation file /mnt/f/temp_folder/toga_out/query_annotation.bed...\n",
      "make_query_isoforms: got 0 unique transcripts annotated in query\n",
      "make_query_isoforms: got data for 0 exons in these trancscripts\n",
      "make_query_isoforms: splitting 0 into buckets based on their chromosome/scaffold and strand\n",
      "make_query_isoforms: got 0 unique chromosome/scaffold combinations\n",
      "make_query_isoforms: Building a graph where nodes are query exons, and edges indicate the fact that their coordinates intersect. Needed to identify which annotated transcripts intersect.\n",
      "make_query_isoforms: identified 0 connected components in the graph\n",
      "make_query_isoforms: parsing components data to identify query genes\n",
      "make_query_isoforms: saving query isoforms data to /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "make_query_isoforms: saving coordinates of inferred genes to /mnt/f/temp_folder/toga_out/query_gene_spans.bed\n",
      "Calling orthology types mapping step...\n",
      "orthology_mapping: called with the following parameters:\n",
      "* ref_bed: /mnt/f/temp_folder/toga_out/temp/toga_filt_ref_annot.bed\n",
      "* que_bed: /mnt/f/temp_folder/toga_out/query_annotation.bed\n",
      "* out: /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "* ref_iso: None\n",
      "* que_iso: /mnt/f/temp_folder/toga_out/query_isoforms.tsv\n",
      "* paralogs_arg: /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "* loss_data: /mnt/f/temp_folder/toga_out/loss_summ_data.tsv\n",
      "* save_skipped: /mnt/f/temp_folder/toga_out/ref_orphan_transcripts.txt\n",
      "* orth_scores_arg: /mnt/f/temp_folder/toga_out/orthology_scores.tsv\n",
      "orthology_mapping: extracted 1 paralogous projections IDs from /mnt/f/temp_folder/toga_out/temp/paralogs.txt\n",
      "orthology_mapping: extracted orthology scores for 1 projections\n",
      "orthology_mapping: got data for 1 reference transcripts\n",
      "orthology_mapping: got data for 1 transcripts annotated in query\n",
      "orthology_mapping: got gene loss classifications for 1 projections in query\n",
      "orthology_mapping: filtered out query transcripts that have loss class not in {'PI', 'I', 'UL'}; resulted in 0 query transcripts to consider\n",
      "orthology_mapping: processed reference transcripts, got data for 1 genes and 1 transcripts\n",
      "orthology_mapping: processed query transcripts, got data for 0 genes and 0 transcripts\n",
      "orthology_mapping: mapped 0 reference transcripts to respective 0 query transcripts\n",
      "orthology_mapping: creating a mapping between reference and query genes...\n",
      "orthology_mapping: added 0 query genes to the orthology graph\n",
      "orthology_mapping: orthology graph contains 1 connected components\n",
      "* assigned class one2zero to node containing reference genes: ['#R#Period'] and query genes: []\n",
      "orthology_mapping: Extracted 1 orthology components in total\n",
      "orthology_mapping: Orthology class sizes:\n",
      "* one2zero: 1\n",
      "orthology_mapping: saving the results to /mnt/f/temp_folder/toga_out/orthology_classification.tsv\n",
      "\n",
      "\n",
      "#### STEP 11: Cleanup: merge parallel steps output files\n",
      "TOGA pipeline is done in 0:01:57.194515\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from Bio import SeqIO\n",
    "\n",
    "location_of_raw_files = \"/mnt/h/My Drive/Circadian Rhythm Genes Project/6.Period Exon Analysis/\"\n",
    "\n",
    "list_of_queries_species = os.listdir(f\"{location_of_raw_files}/4.Pierinae/2.TOGA/1.Query_genomes\")\n",
    "if \"desktop.ini\" in list_of_queries_species:\n",
    "    list_of_queries_species.remove(\"desktop.ini\")\n",
    "list_of_target_species = os.listdir(f\"{location_of_raw_files}/4.Pierinae/2.TOGA/2.Target_genomes\")\n",
    "if \"desktop.ini\" in list_of_target_species:\n",
    "    list_of_target_species.remove(\"desktop.ini\")\n",
    "    \n",
    "for query_species in list_of_queries_species:\n",
    "    for target_species in list_of_target_species:\n",
    "\n",
    "        list_of_folders_in_f  = os.listdir(\"/mnt/f\")\n",
    "        # print(list_of_folders_in_f)\n",
    "        if \"temp_folder\" in list_of_folders_in_f:\n",
    "            subprocess.run(\"rm -r /mnt/f/temp_folder/\", shell = True)\n",
    "            os.mkdir(\"/mnt/f/temp_folder\")\n",
    "\n",
    "        # #copy target genome\n",
    "        subprocess.run(f\"cp '{location_of_raw_files}/4.Pierinae/2.TOGA/1.Query_genomes/{query_species}/query_genome.fa' '/mnt/f/temp_folder'\", shell = True)\n",
    "        subprocess.run(f\"cp '{location_of_raw_files}/4.Pierinae/2.TOGA/2.Target_genomes/{target_species}/target_genome.fa' '/mnt/f/temp_folder'\", shell = True)\n",
    "\n",
    "        Process_genome_name(\"/mnt/f/temp_folder/query_genome.fa\")\n",
    "        Process_genome_name(\"/mnt/f/temp_folder/target_genome.fa\")\n",
    "\n",
    "\n",
    "        subprocess.run(\"python3.11 /mnt/f/make_lastz_chains/make_chains.py target query /mnt/f/temp_folder/query_genome.fa /mnt/f/temp_folder/target_genome.fa --pd /mnt/f/temp_folder/out -f\", shell = True)\n",
    "\n",
    "        subprocess.run(\"gzip -d /mnt/f/temp_folder/out/target.query.final.chain.gz\", shell = True)\n",
    "\n",
    "        # # subprocess.run(f\"cp /mnt/f/temp_folder/out/target.query.final.chain '{location_of_raw_files}/4.Pierinae/2.TOGA/2.Target_genomes/Anthocharis_cardamines/'\", shell = True)\n",
    "\n",
    "        subprocess.run(f'/mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit /mnt/f/temp_folder/query_genome.fa  /mnt/f/temp_folder/query_genome.2bit', shell = True)\n",
    "        subprocess.run(f'/mnt/f/make_lastz_chains/HL_kent_binaries/faToTwoBit /mnt/f/temp_folder/target_genome.fa  /mnt/f/temp_folder/target_genome.2bit', shell = True)\n",
    "\n",
    "\n",
    "        subprocess.run(f'cp \"{location_of_raw_files}/4.Pierinae/2.TOGA/1.Query_genomes/{query_species}/gff_fragment.bed\"  \"/mnt/f/temp_folder\"', shell = True)\n",
    "\n",
    "        subprocess.run(f\"cd /mnt/f/temp_folder/ \\n python3.11 /mnt/f/TOGA/toga.py /mnt/f/temp_folder/out/target.query.final.chain /mnt/f/temp_folder/gff_fragment.bed /mnt/f/temp_folder/query_genome.2bit  /mnt/f/temp_folder/target_genome.2bit --pn /mnt/f/temp_folder/toga_out --ms\", shell = True)\n",
    "\n",
    "        subprocess.run(f'cp \"/mnt/f/temp_folder/toga_out/query_annotation.bed\" \"{location_of_raw_files}/4.Pierinae/2.TOGA/2.Target_genomes/{target_species}/{query_species}_query_annotation.bed\"', shell = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f272a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR989951\t100022\t100102\tExon_1\n",
      "FR989951\t101400\t101543\tExon_2\n",
      "FR989951\t101637\t101738\tExon_3\n",
      "FR989951\t103595\t103765\tExon_4\n",
      "FR989951\t105804\t105944\tExon_5\n",
      "FR989951\t109667\t109908\tExon_6\n",
      "FR989951\t110517\t110591\tExon_7\n",
      "FR989951\t111444\t111539\tExon_8\n",
      "FR989951\t114426\t114618\tExon_9\n",
      "FR989951\t115445\t115558\tExon_10\n",
      "FR989951\t117465\t117598\tExon_11\n",
      "FR989951\t118086\t118238\tExon_12\n",
      "FR989951\t119167\t119280\tExon_13\n",
      "FR989951\t119389\t119497\tExon_14\n",
      "FR989951\t119989\t120217\tExon_15\n",
      "FR989951\t120717\t120954\tExon_16\n",
      "FR989951\t123565\t123649\tExon_17\n",
      "FR989951\t124372\t124503\tExon_18\n",
      "FR989951\t126945\t127096\tExon_19\n",
      "FR989951\t129396\t129470\tExon_20\n",
      "FR989951\t130243\t130354\tExon_21\n",
      "FR989951\t135702\t135704\tExon_22\n",
      "\n",
      "OU538732\t100049\t100141\tExon_1\n",
      "OU538732\t100407\t100541\tExon_2\n",
      "OU538732\t101969\t102070\tExon_3\n",
      "OU538732\t102880\t102936\tExon_4\n",
      "OU538732\t103607\t103762\tExon_5\n",
      "OU538732\t104799\t104995\tExon_6\n",
      "OU538732\t105872\t105955\tExon_7\n",
      "OU538732\t107426\t107588\tExon_8\n",
      "OU538732\t107840\t107980\tExon_9\n",
      "OU538732\t108954\t109072\tExon_10\n",
      "OU538732\t109606\t109755\tExon_11\n",
      "OU538732\t110704\t110817\tExon_12\n",
      "OU538732\t111159\t111246\tExon_13\n",
      "OU538732\t111727\t111965\tExon_14\n",
      "OU538732\t112466\t112683\tExon_15\n",
      "OU538732\t113432\t113524\tExon_16\n",
      "OU538732\t113985\t114119\tExon_17\n",
      "OU538732\t115529\t115593\tExon_18\n",
      "OU538732\t117423\t117589\tExon_19\n",
      "OU538732\t118469\t118555\tExon_20\n",
      "OU538732\t118736\t118750\tExon_21\n",
      "\n",
      "OX637275\t100001\t100108\tExon_1\n",
      "OX637275\t100900\t101043\tExon_2\n",
      "OX637275\t101440\t101541\tExon_3\n",
      "OX637275\t102270\t102479\tExon_4\n",
      "OX637275\t103004\t103138\tExon_5\n",
      "OX637275\t104117\t104359\tExon_6\n",
      "OX637275\t104868\t104948\tExon_7\n",
      "OX637275\t105958\t106141\tExon_8\n",
      "OX637275\t107635\t107781\tExon_9\n",
      "OX637275\t108396\t108498\tExon_10\n",
      "OX637275\t108723\t108814\tExon_11\n",
      "OX637275\t109404\t109553\tExon_12\n",
      "OX637275\t110032\t110151\tExon_13\n",
      "OX637275\t110324\t110423\tExon_14\n",
      "OX637275\t110659\t110887\tExon_15\n",
      "OX637275\t111448\t111664\tExon_16\n",
      "OX637275\t112080\t112125\tExon_17\n",
      "OX637275\t112688\t112825\tExon_18\n",
      "OX637275\t112926\t112970\tExon_19\n",
      "OX637275\t114175\t114329\tExon_20\n",
      "OX637275\t114439\t114522\tExon_21\n",
      "OX637275\t115470\t115478\tExon_22\n",
      "\n",
      "NC_059680\t98094\t98145\tExon_1\n",
      "NC_059680\t99974\t100091\tExon_2\n",
      "NC_059680\t100623\t100706\tExon_3\n",
      "NC_059680\t102458\t102621\tExon_4\n",
      "NC_059680\t104384\t104481\tExon_5\n",
      "NC_059680\t104916\t104963\tExon_6\n",
      "NC_059680\t105168\t105299\tExon_7\n",
      "NC_059680\t107023\t107098\tExon_8\n",
      "NC_059680\t107505\t107726\tExon_9\n",
      "NC_059680\t107959\t108187\tExon_10\n",
      "NC_059680\t108642\t108753\tExon_11\n",
      "NC_059680\t109414\t109557\tExon_12\n",
      "NC_059680\t112557\t112706\tExon_13\n",
      "NC_059680\t113382\t113518\tExon_14\n",
      "NC_059680\t114107\t114223\tExon_15\n",
      "NC_059680\t114772\t114912\tExon_16\n",
      "NC_059680\t115499\t115676\tExon_17\n",
      "NC_059680\t117000\t117147\tExon_18\n",
      "NC_059680\t117459\t117556\tExon_19\n",
      "NC_059680\t117667\t117774\tExon_20\n",
      "NC_059680\t117871\t118112\tExon_21\n",
      "NC_059680\t121084\t121239\tExon_22\n",
      "NC_059680\t121757\t121909\tExon_23\n",
      "NC_059680\t122661\t122804\tExon_24\n",
      "NC_059680\t123041\t123142\tExon_25\n",
      "NC_059680\t123666\t123833\tExon_26\n",
      "NC_059680\t127414\t127423\tExon_27\n",
      "\n",
      "CM054800\t97134\t97155\tExon_1\n",
      "CM054800\t99974\t100085\tExon_2\n",
      "CM054800\t101101\t101184\tExon_3\n",
      "CM054800\t106056\t106219\tExon_4\n",
      "CM054800\t107799\t107902\tExon_5\n",
      "CM054800\t108135\t108182\tExon_6\n",
      "CM054800\t108469\t108600\tExon_7\n",
      "CM054800\t110932\t111007\tExon_8\n",
      "CM054800\t111393\t111608\tExon_9\n",
      "CM054800\t111834\t112081\tExon_10\n",
      "CM054800\t112599\t112704\tExon_11\n",
      "CM054800\t112957\t113070\tExon_12\n",
      "CM054800\t115640\t115790\tExon_13\n",
      "CM054800\t116142\t116290\tExon_14\n",
      "CM054800\t116340\t116453\tExon_15\n",
      "CM054800\t116901\t117041\tExon_16\n",
      "CM054800\t117405\t117582\tExon_17\n",
      "CM054800\t117989\t118124\tExon_18\n",
      "CM054800\t118448\t118545\tExon_19\n",
      "CM054800\t118664\t118786\tExon_20\n",
      "CM054800\t118901\t119199\tExon_21\n",
      "CM054800\t121775\t121930\tExon_22\n",
      "CM054800\t122423\t122588\tExon_23\n",
      "CM054800\t123212\t123310\tExon_24\n",
      "CM054800\t123861\t123965\tExon_25\n",
      "CM054800\t124453\t124608\tExon_26\n",
      "CM054800\t125527\t125534\tExon_27\n",
      "\n",
      "CAVNZK010000320\t100001\t100099\tExon_1\n",
      "CAVNZK010000320\t101471\t101638\tExon_2\n",
      "CAVNZK010000320\t102464\t102565\tExon_3\n",
      "CAVNZK010000320\t102792\t102899\tExon_4\n",
      "CAVNZK010000320\t103806\t103958\tExon_5\n",
      "CAVNZK010000320\t104586\t104723\tExon_6\n",
      "CAVNZK010000320\t105188\t105425\tExon_7\n",
      "CAVNZK010000320\t105548\t105655\tExon_8\n",
      "CAVNZK010000320\t105791\t105903\tExon_9\n",
      "CAVNZK010000320\t106649\t106784\tExon_10\n",
      "CAVNZK010000320\t107231\t107408\tExon_11\n",
      "CAVNZK010000320\t108718\t108858\tExon_12\n",
      "CAVNZK010000320\t110061\t110195\tExon_13\n",
      "CAVNZK010000320\t110281\t110444\tExon_14\n",
      "CAVNZK010000320\t111681\t111830\tExon_15\n",
      "CAVNZK010000320\t113453\t113575\tExon_16\n",
      "CAVNZK010000320\t114302\t114404\tExon_17\n",
      "CAVNZK010000320\t114811\t115049\tExon_18\n",
      "CAVNZK010000320\t115633\t115848\tExon_19\n",
      "CAVNZK010000320\t116253\t116328\tExon_20\n",
      "CAVNZK010000320\t118788\t118922\tExon_21\n",
      "CAVNZK010000320\t119161\t119205\tExon_22\n",
      "CAVNZK010000320\t119643\t119737\tExon_23\n",
      "CAVNZK010000320\t121291\t121445\tExon_24\n",
      "CAVNZK010000320\t124583\t124666\tExon_25\n",
      "CAVNZK010000320\t125078\t125189\tExon_26\n",
      "CAVNZK010000320\t126819\t126821\tExon_27\n",
      "\n",
      "NC_062259\t98336\t98357\tExon_1\n",
      "NC_062259\t99974\t100085\tExon_2\n",
      "NC_062259\t100619\t100702\tExon_3\n",
      "NC_062259\t102974\t103128\tExon_4\n",
      "NC_062259\t104311\t104402\tExon_5\n",
      "NC_062259\t104785\t104850\tExon_6\n",
      "NC_062259\t105100\t105231\tExon_7\n",
      "NC_062259\t107936\t108011\tExon_8\n",
      "NC_062259\t108424\t108639\tExon_9\n",
      "NC_062259\t109896\t110134\tExon_10\n",
      "NC_062259\t110577\t110679\tExon_11\n",
      "NC_062259\t111382\t111504\tExon_12\n",
      "NC_062259\t112481\t112630\tExon_13\n",
      "NC_062259\t113468\t113617\tExon_14\n",
      "NC_062259\t114396\t114530\tExon_15\n",
      "NC_062259\t115038\t115181\tExon_16\n",
      "NC_062259\t115887\t116064\tExon_17\n",
      "NC_062259\t116376\t116520\tExon_18\n",
      "NC_062259\t116883\t116995\tExon_19\n",
      "NC_062259\t117121\t117237\tExon_20\n",
      "NC_062259\t117351\t117589\tExon_21\n",
      "NC_062259\t120634\t120813\tExon_22\n",
      "NC_062259\t121379\t121537\tExon_23\n",
      "NC_062259\t122858\t123010\tExon_24\n",
      "NC_062259\t124588\t124701\tExon_25\n",
      "NC_062259\t125859\t126026\tExon_26\n",
      "NC_062259\t127283\t127292\tExon_27\n",
      "\n",
      "NC_059534\t98307\t98328\tExon_1\n",
      "NC_059534\t99974\t100085\tExon_2\n",
      "NC_059534\t101110\t101193\tExon_3\n",
      "NC_059534\t104147\t104316\tExon_4\n",
      "NC_059534\t105548\t105645\tExon_5\n",
      "NC_059534\t105852\t105899\tExon_6\n",
      "NC_059534\t106246\t106377\tExon_7\n",
      "NC_059534\t108031\t108106\tExon_8\n",
      "NC_059534\t108425\t108649\tExon_9\n",
      "NC_059534\t108863\t109086\tExon_10\n",
      "NC_059534\t109482\t109607\tExon_11\n",
      "NC_059534\t109966\t110085\tExon_12\n",
      "NC_059534\t112074\t112223\tExon_13\n",
      "NC_059534\t112639\t112772\tExon_14\n",
      "NC_059534\t112854\t112961\tExon_15\n",
      "NC_059534\t113810\t113950\tExon_16\n",
      "NC_059534\t114496\t114673\tExon_17\n",
      "NC_059534\t116539\t116674\tExon_18\n",
      "NC_059534\t117004\t117101\tExon_19\n",
      "NC_059534\t117228\t117341\tExon_20\n",
      "NC_059534\t117456\t117709\tExon_21\n",
      "NC_059534\t120298\t120435\tExon_22\n",
      "NC_059534\t120978\t121136\tExon_23\n",
      "NC_059534\t121810\t121908\tExon_24\n",
      "NC_059534\t122143\t122244\tExon_25\n",
      "NC_059534\t122644\t122811\tExon_26\n",
      "NC_059534\t123579\t123588\tExon_27\n",
      "\n",
      "FR989951\t100007\t100102\tExon_1\n",
      "FR989951\t101637\t101738\tExon_2\n",
      "FR989951\t103583\t103766\tExon_3\n",
      "FR989951\t105804\t105944\tExon_4\n",
      "FR989951\t111453\t111539\tExon_5\n",
      "FR989951\t114444\t114618\tExon_6\n",
      "FR989951\t117465\t117592\tExon_7\n",
      "FR989951\t119383\t119497\tExon_8\n",
      "FR989951\t119998\t120205\tExon_9\n",
      "FR989951\t120731\t120906\tExon_10\n",
      "FR989951\t123565\t123640\tExon_11\n",
      "FR989951\t124372\t124503\tExon_12\n",
      "FR989951\t129396\t129485\tExon_13\n",
      "FR989951\t130240\t130247\tExon_14\n",
      "\n",
      "OU538732\t100055\t100141\tExon_1\n",
      "OU538732\t101969\t102070\tExon_2\n",
      "OU538732\t107426\t107603\tExon_3\n",
      "OU538732\t108930\t109057\tExon_4\n",
      "OU538732\t109644\t109755\tExon_5\n",
      "OU538732\t110704\t110817\tExon_6\n",
      "OU538732\t111135\t111246\tExon_7\n",
      "OU538732\t111727\t111947\tExon_8\n",
      "OU538732\t112511\t112648\tExon_9\n",
      "OU538732\t113985\t114119\tExon_10\n",
      "OU538732\t118487\t118555\tExon_11\n",
      "OU538732\t118736\t118748\tExon_12\n",
      "\n",
      "OX637275\t100001\t100109\tExon_1\n",
      "OX637275\t101440\t101541\tExon_2\n",
      "OX637275\t102023\t102100\tExon_3\n",
      "OX637275\t102270\t102483\tExon_4\n",
      "OX637275\t103004\t103138\tExon_5\n",
      "OX637275\t105976\t106141\tExon_6\n",
      "OX637275\t108681\t108805\tExon_7\n",
      "OX637275\t109452\t109553\tExon_8\n",
      "OX637275\t110053\t110151\tExon_9\n",
      "OX637275\t110303\t110423\tExon_10\n",
      "OX637275\t110724\t110887\tExon_11\n",
      "OX637275\t112041\t112125\tExon_12\n",
      "OX637275\t112712\t112825\tExon_13\n",
      "OX637275\t114439\t114537\tExon_14\n",
      "OX637275\t115467\t115476\tExon_15\n",
      "\n",
      "NC_059680\t100000\t100091\tExon_1\n",
      "NC_059680\t100647\t100706\tExon_2\n",
      "NC_059680\t105159\t105281\tExon_3\n",
      "NC_059680\t107289\t107351\tExon_4\n",
      "NC_059680\t107959\t108217\tExon_5\n",
      "NC_059680\t108642\t108771\tExon_6\n",
      "NC_059680\t109414\t109536\tExon_7\n",
      "NC_059680\t112539\t112676\tExon_8\n",
      "NC_059680\t113382\t113518\tExon_9\n",
      "NC_059680\t115499\t115703\tExon_10\n",
      "NC_059680\t121084\t121239\tExon_11\n",
      "NC_059680\t121747\t121937\tExon_12\n",
      "NC_059680\t123041\t123142\tExon_13\n",
      "NC_059680\t125774\t125800\tExon_14\n",
      "NC_059680\t127413\t127422\tExon_15\n",
      "\n",
      "CM054800\t100000\t100088\tExon_1\n",
      "CM054800\t101110\t101184\tExon_2\n",
      "CM054800\t108469\t108582\tExon_3\n",
      "CM054800\t111846\t112057\tExon_4\n",
      "CM054800\t112599\t112710\tExon_5\n",
      "CM054800\t112957\t113070\tExon_6\n",
      "CM054800\t115634\t115749\tExon_7\n",
      "CM054800\t116142\t116275\tExon_8\n",
      "CM054800\t117405\t117562\tExon_9\n",
      "CM054800\t121775\t121930\tExon_10\n",
      "CM054800\t122431\t122600\tExon_11\n",
      "CM054800\t123861\t123965\tExon_12\n",
      "CM054800\t124470\t124472\tExon_13\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAVNZK010000320\t100001\t100100\tExon_1\n",
      "CAVNZK010000320\t101580\t101609\tExon_2\n",
      "CAVNZK010000320\t102464\t102565\tExon_3\n",
      "CAVNZK010000320\t103778\t103968\tExon_4\n",
      "CAVNZK010000320\t107251\t107408\tExon_5\n",
      "CAVNZK010000320\t110311\t110444\tExon_6\n",
      "CAVNZK010000320\t111709\t111836\tExon_7\n",
      "CAVNZK010000320\t113453\t113575\tExon_8\n",
      "CAVNZK010000320\t114275\t114404\tExon_9\n",
      "CAVNZK010000320\t114811\t115049\tExon_10\n",
      "CAVNZK010000320\t115678\t115815\tExon_11\n",
      "CAVNZK010000320\t118806\t118928\tExon_12\n",
      "CAVNZK010000320\t124583\t124669\tExon_13\n",
      "CAVNZK010000320\t125075\t125083\tExon_14\n",
      "\n",
      "NC_062259\t100000\t100088\tExon_1\n",
      "NC_062259\t100607\t100702\tExon_2\n",
      "NC_062259\t105100\t105213\tExon_3\n",
      "NC_062259\t108074\t108149\tExon_4\n",
      "NC_062259\t109896\t110134\tExon_5\n",
      "NC_062259\t110577\t110706\tExon_6\n",
      "NC_062259\t111382\t111504\tExon_7\n",
      "NC_062259\t112475\t112602\tExon_8\n",
      "NC_062259\t113468\t113601\tExon_9\n",
      "NC_062259\t115887\t116044\tExon_10\n",
      "NC_062259\t120688\t120813\tExon_11\n",
      "NC_062259\t121375\t121570\tExon_12\n",
      "NC_062259\t124588\t124701\tExon_13\n",
      "NC_062259\t125888\t125917\tExon_14\n",
      "NC_062259\t127285\t127294\tExon_15\n",
      "\n",
      "NC_059534\t100000\t100088\tExon_1\n",
      "NC_059534\t101134\t101193\tExon_2\n",
      "NC_059534\t106219\t106334\tExon_3\n",
      "NC_059534\t108031\t108106\tExon_4\n",
      "NC_059534\t108875\t109086\tExon_5\n",
      "NC_059534\t109482\t109611\tExon_6\n",
      "NC_059534\t109966\t110085\tExon_7\n",
      "NC_059534\t112092\t112195\tExon_8\n",
      "NC_059534\t112639\t112772\tExon_9\n",
      "NC_059534\t114496\t114682\tExon_10\n",
      "NC_059534\t117025\t117101\tExon_11\n",
      "NC_059534\t120298\t120414\tExon_12\n",
      "NC_059534\t120968\t121136\tExon_13\n",
      "NC_059534\t122143\t122244\tExon_14\n",
      "NC_059534\t123174\t123209\tExon_15\n",
      "NC_059534\t123578\t123587\tExon_16\n",
      "\n",
      "FR989951\t100025\t100102\tExon_1\n",
      "FR989951\t101637\t101738\tExon_2\n",
      "FR989951\t102299\t102400\tExon_3\n",
      "FR989951\t103583\t103765\tExon_4\n",
      "FR989951\t105804\t105944\tExon_5\n",
      "FR989951\t111438\t111539\tExon_6\n",
      "FR989951\t114429\t114618\tExon_7\n",
      "FR989951\t117465\t117592\tExon_8\n",
      "FR989951\t118129\t118238\tExon_9\n",
      "FR989951\t119383\t119497\tExon_10\n",
      "FR989951\t119989\t120157\tExon_11\n",
      "FR989951\t120731\t120906\tExon_12\n",
      "FR989951\t123565\t123640\tExon_13\n",
      "FR989951\t124372\t124503\tExon_14\n",
      "FR989951\t129396\t129485\tExon_15\n",
      "FR989951\t130240\t130247\tExon_16\n",
      "\n",
      "OU538732\t100052\t100141\tExon_1\n",
      "OU538732\t100416\t100520\tExon_2\n",
      "OU538732\t101969\t102070\tExon_3\n",
      "OU538732\t102880\t102927\tExon_4\n",
      "OU538732\t103610\t103745\tExon_5\n",
      "OU538732\t104886\t104995\tExon_6\n",
      "OU538732\t107432\t107603\tExon_7\n",
      "OU538732\t108930\t109057\tExon_8\n",
      "OU538732\t109646\t109755\tExon_9\n",
      "OU538732\t110704\t110817\tExon_10\n",
      "OU538732\t111135\t111246\tExon_11\n",
      "OU538732\t111727\t111947\tExon_12\n",
      "OU538732\t113446\t113521\tExon_13\n",
      "OU538732\t113985\t114119\tExon_14\n",
      "OU538732\t118478\t118555\tExon_15\n",
      "OU538732\t118736\t118748\tExon_16\n",
      "\n",
      "OX637275\t100001\t100109\tExon_1\n",
      "OX637275\t100924\t101043\tExon_2\n",
      "OX637275\t101440\t101541\tExon_3\n",
      "OX637275\t102270\t102446\tExon_4\n",
      "OX637275\t105964\t106141\tExon_5\n",
      "OX637275\t108681\t108805\tExon_6\n",
      "OX637275\t109467\t109553\tExon_7\n",
      "OX637275\t110016\t110151\tExon_8\n",
      "OX637275\t110303\t110423\tExon_9\n",
      "OX637275\t110692\t110887\tExon_10\n",
      "OX637275\t111476\t111664\tExon_11\n",
      "OX637275\t112041\t112125\tExon_12\n",
      "OX637275\t112712\t112825\tExon_13\n",
      "OX637275\t114439\t114537\tExon_14\n",
      "OX637275\t115467\t115476\tExon_15\n",
      "\n",
      "NC_059680\t100000\t100091\tExon_1\n",
      "NC_059680\t100617\t100706\tExon_2\n",
      "NC_059680\t102470\t102570\tExon_3\n",
      "NC_059680\t104384\t104481\tExon_4\n",
      "NC_059680\t105159\t105281\tExon_5\n",
      "NC_059680\t107023\t107114\tExon_6\n",
      "NC_059680\t107544\t107726\tExon_7\n",
      "NC_059680\t108642\t108771\tExon_8\n",
      "NC_059680\t109414\t109559\tExon_9\n",
      "NC_059680\t112539\t112673\tExon_10\n",
      "NC_059680\t113382\t113518\tExon_11\n",
      "NC_059680\t115499\t115703\tExon_12\n",
      "NC_059680\t121765\t121906\tExon_13\n",
      "NC_059680\t122664\t122804\tExon_14\n",
      "NC_059680\t123041\t123142\tExon_15\n",
      "NC_059680\t123705\t123809\tExon_16\n",
      "NC_059680\t127413\t127422\tExon_17\n",
      "\n",
      "CM054800\t100000\t100088\tExon_1\n",
      "CM054800\t101098\t101184\tExon_2\n",
      "CM054800\t108469\t108582\tExon_3\n",
      "CM054800\t110932\t111043\tExon_4\n",
      "CM054800\t111846\t112057\tExon_5\n",
      "CM054800\t112599\t112710\tExon_6\n",
      "CM054800\t112957\t113093\tExon_7\n",
      "CM054800\t115634\t115749\tExon_8\n",
      "CM054800\t116142\t116275\tExon_9\n",
      "CM054800\t117405\t117609\tExon_10\n",
      "CM054800\t120681\t120788\tExon_11\n",
      "CM054800\t122429\t122600\tExon_12\n",
      "CM054800\t123231\t123310\tExon_13\n",
      "CM054800\t123861\t123965\tExon_14\n",
      "CM054800\t124480\t124590\tExon_15\n",
      "CM054800\t125551\t125555\tExon_16\n",
      "\n",
      "CAVNZK010000320\t99998\t100100\tExon_1\n",
      "CAVNZK010000320\t101495\t101599\tExon_2\n",
      "CAVNZK010000320\t102464\t102565\tExon_3\n",
      "CAVNZK010000320\t102792\t102851\tExon_4\n",
      "CAVNZK010000320\t103778\t103950\tExon_5\n",
      "CAVNZK010000320\t104586\t104723\tExon_6\n",
      "CAVNZK010000320\t107204\t107408\tExon_7\n",
      "CAVNZK010000320\t110311\t110444\tExon_8\n",
      "CAVNZK010000320\t111709\t111836\tExon_9\n",
      "CAVNZK010000320\t113453\t113575\tExon_10\n",
      "CAVNZK010000320\t114275\t114404\tExon_11\n",
      "CAVNZK010000320\t114811\t115049\tExon_12\n",
      "CAVNZK010000320\t115678\t115815\tExon_13\n",
      "CAVNZK010000320\t116237\t116328\tExon_14\n",
      "CAVNZK010000320\t118806\t118928\tExon_15\n",
      "CAVNZK010000320\t124583\t124669\tExon_16\n",
      "CAVNZK010000320\t125075\t125083\tExon_17\n",
      "\n",
      "NC_062259\t100000\t100088\tExon_1\n",
      "NC_062259\t100607\t100702\tExon_2\n",
      "NC_062259\t105100\t105213\tExon_3\n",
      "NC_062259\t107936\t108027\tExon_4\n",
      "NC_062259\t108457\t108639\tExon_5\n",
      "NC_062259\t109896\t110134\tExon_6\n",
      "NC_062259\t110577\t110706\tExon_7\n",
      "NC_062259\t111382\t111527\tExon_8\n",
      "NC_062259\t112475\t112602\tExon_9\n",
      "NC_062259\t113468\t113601\tExon_10\n",
      "NC_062259\t115887\t116091\tExon_11\n",
      "NC_062259\t121375\t121570\tExon_12\n",
      "NC_062259\t122906\t123010\tExon_13\n",
      "NC_062259\t124588\t124701\tExon_14\n",
      "NC_062259\t125898\t125999\tExon_15\n",
      "NC_062259\t127307\t127314\tExon_16\n",
      "\n",
      "NC_059534\t100000\t100088\tExon_1\n",
      "NC_059534\t101107\t101193\tExon_2\n",
      "NC_059534\t106219\t106334\tExon_3\n",
      "NC_059534\t108031\t108119\tExon_4\n",
      "NC_059534\t108875\t109086\tExon_5\n",
      "NC_059534\t109482\t109611\tExon_6\n",
      "NC_059534\t109966\t110057\tExon_7\n",
      "NC_059534\t112080\t112195\tExon_8\n",
      "NC_059534\t112639\t112772\tExon_9\n",
      "NC_059534\t114496\t114682\tExon_10\n",
      "NC_059534\t120160\t120254\tExon_11\n",
      "NC_059534\t120990\t121136\tExon_12\n",
      "NC_059534\t121810\t121908\tExon_13\n",
      "NC_059534\t122143\t122244\tExon_14\n",
      "NC_059534\t122683\t122695\tExon_15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query_species in list_of_queries_species:\n",
    "    for target_species in list_of_target_species:\n",
    "        with open(f\"{location_of_raw_files}/4.Pierinae/2.TOGA/2.Target_genomes/{target_species}/{query_species}_query_annotation.bed\", 'r') as bed_input_file:\n",
    "            bed_input_lines = bed_input_file.readlines()\n",
    "        splitted = (bed_input_lines[0].split(\"\\t\"))\n",
    "        chromosome = splitted[0]\n",
    "        chromosome_start = splitted[1]\n",
    "        chromosome_stop = splitted[2]\n",
    "        output = ''\n",
    "        start_position_list = splitted[11][:-2].split(\",\")\n",
    "        length_list = splitted[10][:-2].split(\",\")\n",
    "#         print(start_position_list, length_list)\n",
    "        for i in range(len(length_list)):\n",
    "            output+= f\"{chromosome}\\t{int(chromosome_start)+int(start_position_list[i])}\\t{int(chromosome_start)+int(start_position_list[i])+int(length_list[i])}\\tExon_{i+1}\\n\"\n",
    "\n",
    "        with open(f\"{location_of_raw_files}/4.Pierinae/2.TOGA/2.Target_genomes/{target_species}/{query_species}_query_annotation_processed.bed\", 'w') as out_file:\n",
    "            out_file.write(output)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11ec05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
